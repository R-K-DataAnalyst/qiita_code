# ã¯ã˜ã‚ã«
ä¸–é–“ã¯Pytorchä¸€æŠãªã®ã ã‚ã†ã‹ã€‚ç§ã¯ä¿¡ã˜ãŸã„ã€Keras(Tensorflow)ã®åŠ›ã‚’ã€‚

ã¨ã„ã†ã“ã¨ã§Kerasã§BERTãƒ¢ãƒ‡ãƒ«ã‚’æ‰±ã£ãŸã®ã§å‚™å¿˜éŒ²ã‚’æ®‹ã™ã€‚

ã„ã‚ã„ã‚èª¿ã¹ã¦ã„ã‚‹ã¨ã€huggingfaceã®transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†ã¨ãã¯ã€Pytorchã‚’ä½¿ã£ã¦ã‚‹äººãŒå¤šã„ã¨æ„Ÿã˜ãŸã€‚è‡ªåˆ†ã‚‚[éå»ã®è¨˜äº‹](https://qiita.com/chicken_data_analyst/items/15c0046062c6e016f467)ã§ã¯Pytorchã‚’ä½¿ã£ã¦ã„ãŸã€‚ãŸã æ…£ã‚Œã¦ã„ã‚‹ã®ã¯Kerasãªã®ã§ã€Kerasã§transformersã®ãƒ¢ãƒ‡ãƒ«ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«ãªã£ã¦ãŠããŸã„ãªã¨æ€ã„ã€ãŠå‹‰å¼·ã—ãŸã¨ã„ã†çµŒç·¯ã€‚
å…ˆäººãŸã¡ã®ã„ã‚ã„ã‚ãªçŸ¥è¦‹ã‚’ã¤ã¾ã¿é£Ÿã„ã—ãªãŒã‚‰ãŠå‹‰å¼·ã—ãŸã®ã§ã€è‡ªåˆ†ç”¨ã«ã“ã®è¨˜äº‹ã‚’æ›¸ã„ã¦çŸ¥è¦‹ã‚’1ã¤ã«ã¾ã¨ã‚ãŸã„ã€‚

ä»Šå›æ›¸ãã®ã¯ä»¥ä¸‹2ã¤ã€‚
- Kerasã‚’ä½¿ã£ãŸMaskedLMã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹BERTå­¦ç¿’æ–¹æ³•
- Kerasã‚’ä½¿ã£ãŸBERTã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ–‡æ›¸åˆ†é¡ï¼‰ã®æ–¹æ³•

# å‚è€ƒ
- [[huggingface] masked_language_modeling](https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling)
- [[Keraså…¬å¼] Pretraining BERT with Hugging Face Transformers](https://keras.io/examples/nlp/pretraining_BERT/)
- [[Github] transformers/examples/pytorch/language-modeling
/run_mlm.py](https://github.com/huggingface/transformers/blob/v4.18.0/examples/pytorch/language-modeling/run_mlm.py)
- [ã€Transformerã«ã‚ˆã‚‹è‡ªç„¶è¨€èªå‡¦ç†ã€ã®RoBERTaäº‹å‰è¨“ç·´ã®ã‚³ãƒ¼ãƒ‰ã‚’ã€ãƒ‡ãƒ¼ã‚¿ã‚’huggingface/datasetsã§èª­ã¿è¾¼ã‚€ã‚ˆã†ã«æ›¸ãç›´ã™](https://nikkie-ftnext.hatenablog.com/entry/replace-linebylinetextdataset-datasets-library)
- [[huggingface] Main classesï¼što_tf_dataset()](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset)
- [[Github] TFBertModel.predict() and TFBertModel() return different results. #16160](https://github.com/huggingface/transformers/issues/16160)
- [[stackoverflow] batch_size in tf model.fit() vs. batch_size in tf.data.Dataset](https://stackoverflow.com/questions/62670041/batch-size-in-tf-model-fit-vs-batch-size-in-tf-data-dataset)
- [huggingface Tokenizer ã® tokenize, encode, encode_plus ãªã©ã®é•ã„](https://zenn.dev/hellorusk/articles/7fd588cae5b173)
- [kaggleã§ã‚ˆãä½¿ã†äº¤å·®æ¤œè¨¼ãƒ†ãƒ³ãƒ—ãƒ¬(Keraså‘ã‘)](https://zenn.dev/monda/articles/kaggle-cv-template)
- [TensorFlow(Keras)ã¨BERTã§æ–‡ç« åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹æ–¹æ³•](https://htomblog.com/python-tensorflowbert)
- [[Github] Transfer learning & fine-tuning](https://github.com/keras-team/keras-io/blob/master/guides/ipynb/transfer_learning.ipynb)
- [[Github] Difference between CLS hidden state and pooled_output #7540](https://github.com/huggingface/transformers/issues/7540)
- [TensorFlowã§ä½¿ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ©Ÿèƒ½ãŒå¼·ã‹ã£ãŸè©±](https://qiita.com/Suguru_Toyohara/items/820b0dad955ecd91c7f3)

# çµè«–
ã•ã‚‰ã£ã¨çµè«–ã‚’æ›¸ãã€‚
- transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¯ãƒ©ã‚¹ã¯ã€```TFAutoModel```ã‚„```TFBertForMaskedLM```ã®ã‚ˆã†ã«TFãŒä»˜ã„ãŸã‚¯ãƒ©ã‚¹ã‚’ä½¿ã†
- MaskedLMã‚’å®Ÿæ–½ã™ã‚‹æ™‚ã€DatasetDictã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€```to_tf_dataset()```ã‚’é©ç”¨ã™ã‚‹ã¨Keras(Tensorflow)ã§æ‰±ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚‹
- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ™‚ã¯ã€```TFAutoModel.from_pretrained()```ã§äº‹å‰å­¦ç¿’æ¸ˆã¿ã®BERTãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã€[CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€çµ‚éš ã‚Œå±¤```<BERT MODEL OUTPUT>.last_hidden_state[:,0,:]```ã‚’å¾Œç¶šã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ã¤ãªã

# Kerasã‚’ä½¿ã£ãŸMaskedLMã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹BERTå­¦ç¿’æ–¹æ³•
Kerasã§MaskedLMã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹å­¦ç¿’ã‚’å®Ÿæ–½ã™ã‚‹æ–¹æ³•ã‚’æ›¸ãã€‚

ã¾ãšå¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’importã€‚ï¼ˆTPUã‚’ä½¿ã£ãŸã®ã§ã€TPUã‚’ä½¿ã†è¨­å®šã‚‚å…¥ã‚Œã¦ã„ã‚‹ã€‚ï¼‰
```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import glob
from tqdm import tqdm
import scipy
import gc
import pickle
import os
import collections
import random
import string
import re
import sklearn

import tensorflow as tf  # 2.13.0

import transformers  # 4.35.2
from transformers import AutoTokenizer, TFAutoModel, TFBertForMaskedLM, TFBertForPreTraining
from transformers import BertConfig
from transformers import DataCollatorForLanguageModeling
from transformers import AdamWeightDecay

from datasets import load_dataset
print(tf.__version__)
print(transformers.__version__)

tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)
```

ãƒˆãƒ¼ã‚¯ãƒ³é•·ã€ãƒã‚¹ã‚¯ã™ã‚‹ç¢ºç‡ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®åå‰ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ åã®å®šç¾©ã€‚[Keraså…¬å¼](https://keras.io/examples/nlp/pretraining_BERT/)ã®ãƒšãƒ¼ã‚¸ã‚’å‚ç…§ã—ã¦ã„ã‚‹ã€‚
```python
MAX_LENGTH = 512  # Maximum number of tokens in an input sample after padding
MLM_PROB = 0.2  # Probability with which tokens are masked in MLM
MODEL_CHECKPOINT = "bert-base-uncased"  # Name of pretrained model from ğŸ¤— Model Hub
text_column_name = 'text'
```

ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã€‚Wikiã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ã€‚DatasetDictã‚¯ãƒ©ã‚¹ã¨ã—ã¦èª­ã¿è¾¼ã¾ã‚Œã‚‹ã€‚
```python
raw_datasets = load_dataset("wikitext", "wikitext-2-raw-v1")
raw_datasets
```
![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/542929/c3682204-d207-a54f-10b3-8a6bc1d22c3b.png)

ã¡ãªã¿ã«è‡ªå‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’DatasetDictã‚¯ãƒ©ã‚¹ã¨ã—ã¦èª­ã¿è¾¼ã‚€å ´åˆã€å‰å‡¦ç†ãªã©ã—ãŸå¾Œ1åº¦ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¦ã‹ã‚‰load_datasetã§èª­ã¿è¾¼ã‚ã°ã„ã„ã€‚
```python
#### ä¾‹ ####
# å‰å‡¦ç†ã—ãŸãƒ‡ãƒ¼ã‚¿
data['text'].to_csv('tmp.txt', index=False, header=False)
data_valid['text'].to_csv('tmp_valid.txt', index=False, header=False)

dataset_files = {
    "train": ["tmp.txt"],
    "validation": ["tmp_valid.txt"],
}
raw_datasets = load_dataset(text_column_name, data_files=dataset_files)
```

æ¬¡ã«tokenizerã¨Data Collatorã‚’å®šç¾©ã€‚```DataCollatorForLanguageModeling```ã«ã‚ˆã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚¹ã‚¯å‡¦ç†ã®å†…å®¹ã‚’å®šç¾©ã§ãã‚‹ã€‚
```python
tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB, return_tensors="tf")
```

æ¬¡ã«ã€æ–‡æ›¸ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ãƒŠã‚¤ã‚ºã—ã¦ã€MaskedLMã‚¿ã‚¹ã‚¯ã§å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã®datasetã«åŠ å·¥ã™ã‚‹ã€‚
ä»¥å‰ã¯```LineByLineTextDataset```ã‚’ä½¿ã£ã¦ã„ãŸãŒã€ä»Šã¯éæ¨å¥¨ã‚‰ã—ãï¼ˆ[ã€Transformerã«ã‚ˆã‚‹è‡ªç„¶è¨€èªå‡¦ç†ã€ã®RoBERTaäº‹å‰è¨“ç·´ã®ã‚³ãƒ¼ãƒ‰ã‚’ã€ãƒ‡ãƒ¼ã‚¿ã‚’huggingface/datasetsã§èª­ã¿è¾¼ã‚€ã‚ˆã†ã«æ›¸ãç›´ã™](https://nikkie-ftnext.hatenablog.com/entry/replace-linebylinetextdataset-datasets-library)ï¼‰ã€datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†å¿…è¦ãŒã‚ã‚‹ã€‚
[transformersã®Github](https://github.com/huggingface/transformers/blob/v4.18.0/examples/pytorch/language-modeling/run_mlm.py)ã‹ã‚‰ã€```tokenize_function```ã¨ã„ã†é–¢æ•°ã‚’æ‹å€Ÿã—ã¦mapã‚’é©ç”¨ã™ã‚Œã°ã€ãã‚ŒãŒå¯èƒ½ã«ãªã‚‹ã€‚
```python
def tokenize_function(examples):
    # Remove empty lines
    examples[text_column_name] = [
        line for line in examples[text_column_name] if len(line) > 0 and not line.isspace()
    ]
    return tokenizer(
        examples[text_column_name],
        padding=True,
        truncation=True,
        max_length=MAX_LENGTH,
        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it
        # receives the `special_tokens_mask`.
        return_special_tokens_mask=True,
    )

tokenized_datasets = raw_datasets.map(
    tokenize_function,
    batched=True,  # https://discuss.huggingface.co/t/why-use-batched-true-in-map-function/18042
    num_proc=None,
    remove_columns=[text_column_name],
    load_from_cache_file=True,
    desc="Running tokenizer on dataset line_by_line",
)
tokenized_datasets
```
![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/542929/c8838086-4817-f56e-1281-8e7c7002259d.png)


ã¨ã€ã“ã“ã¾ã§ã„ã‚ã„ã‚å‡¦ç†ã—ã¦ããŸãŒã€ã“ã“ã¾ã§ã®å‡¦ç†ã¯Pytorchã§ã‚‚Keras(Tensorflow)ã§ã‚‚å¤‰ã‚ã‚‰ãªã„ã€‚

æ¬¡ã¯ã€ã“ã“ã¾ã§å‡¦ç†ã—ã¦ããŸãƒ‡ãƒ¼ã‚¿ã‚’Keras(Tensorflow)ç”¨ã«åŠ å·¥ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ãã®ãŸã‚ã«```to_tf_dataset()```é–¢æ•°ã‚’ä½¿ã†ã€‚ï¼ˆ[[huggingface] Main classesï¼što_tf_dataset()](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset)ï¼‰
```to_tf_dataset()```é–¢æ•°ã®ä¸­ã®å¼•æ•°collate_fnã§å…ˆã»ã©å®šç¾©ã—ãŸdata_collatorã‚’æŒ‡å®šã™ã‚‹ã¨ã€ãƒã‚¹ã‚¯å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã«ã‚‚åŠ å·¥ã•ã‚Œã‚‹ã€‚
ã“ã“ã§batch_sizeã‚‚æŒ‡å®šã—ã¦ã„ã‚‹ã®ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã‚‹ã¨ãã®å¼•æ•°ã§ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªããªã‚‹ã€‚ï¼ˆ[[stackoverflow] batch_size in tf model.fit() vs. batch_size in tf.data.Dataset](https://stackoverflow.com/questions/62670041/batch-size-in-tf-model-fit-vs-batch-size-in-tf-data-dataset)ï¼‰
```python
np.object = object  # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹ã®ã§ä»•æ–¹ãªã—ã€‚éæ¨å¥¨ã€‚npã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³<1.24ã«ã™ã‚Œã°ã„ã„ã‚‰ã—ã„ã€‚(https://stackoverflow.com/questions/75069062/module-numpy-has-no-attribute-object)
ds_train = tokenized_datasets["train"].to_tf_dataset(
   columns=['input_ids', 'token_type_ids', 'attention_mask'],
   label_cols=["labels"],
   shuffle=True,
   batch_size=64,
   collate_fn=data_collator,
)
ds_valid = tokenized_datasets["validation"].to_tf_dataset(
   columns=['input_ids', 'token_type_ids', 'attention_mask'],
   label_cols=["labels"],
   shuffle=True,
   batch_size=64,
   collate_fn=data_collator,
)
ds_train
```

ã“ã‚Œã§Tensorflowã§ã‚‚æ‰±ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã«å¤‰ã‚ã£ãŸã®ã§ã€ã‚ã¨ã¯å­¦ç¿’ã•ã›ã‚‹ã ã‘ã€‚
Pytorchã®æ™‚ã¯ã€```BertForMaskedLM```ã§ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã—ã¦ã„ãŸãŒã€Keras(Tensorflow)ã®æ™‚ã¯ã€```TFBertForMaskedLM```ã§å®šç¾©ã™ã‚‹ã€‚
```python
%%time
# TPUç”¨ã®è¨­å®š
# ãƒãƒƒãƒã‚µã‚¤ã‚º
batch_size_per_replica = 64

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°
total_samples = len(tokenized_datasets["train"])
total_samples_v = len(tokenized_datasets["validation"])

# steps_per_epoch ã®è¨ˆç®—
steps_per_epoch = total_samples // batch_size_per_replica
val_steps_per_epoch = total_samples_v // batch_size_per_replica
print(steps_per_epoch, val_steps_per_epoch)
# > 371 38

with tpu_strategy.scope():
    config = BertConfig()
    model = TFBertForMaskedLM(config)
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®è¨­å®š
    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4)
    #optimizer = AdamWeightDecay(learning_rate=1e-3, weight_decay_rate=0.0)
    model.compile(optimizer=optimizer)
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
# batch_sizeã®æŒ‡å®šã¯ã„ã‚‰ãªã„
model.fit(x=ds_train, validation_data=ds_valid, epochs=100
          , callbacks=[callback]
          , steps_per_epoch=steps_per_epoch, validation_steps=val_steps_per_epoch)
# > Wall time: 2h 10min 57s
```
![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/542929/8f560906-d854-f9ae-7e9d-0ac17c653b30.png)

ã¡ã‚ƒã‚“ã¨å­¦ç¿’ã§ãã¦ã„ã‚‹ã‹ç¢ºèªã™ã‚‹ã€‚ä¾‹æ–‡ã®å˜èªã‚’1ã¤[MASK]ã¨ã—ã¦éš ã—ã¦ã€ãã‚Œã‚’å½“ã¦ã‚‰ã‚Œã‚‹ã‹æ¤œè¨¼ã™ã‚‹ã€‚ä»Šå›ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã‹ã‚‰ä¾‹æ–‡ã‚’å–ã‚‹ã€‚ä»¥ä¸‹ã®ä¾‹æ–‡ã®â€historicâ€ã®éƒ¨åˆ†ã‚’[MASK]ã«å¤‰ãˆã‚‹ã€‚
```
galveston is home to six historic districts with over 60 structures listed representing architectural significance
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€â†“
galveston is home to six [MASK] districts with over 60 structures listed representing architectural significance
```
å®Ÿéš›ã«æ¨è«–ã—ã¦ã¿ã‚‹ã€‚ï¼ˆ[[huggingface] masked_language_modeling](https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling)ï¼‰
ã¡ã‚ƒã‚“ã¨"historic"ã‚’æ¨è«–ã§ããŸã€‚
```python
mlm_bert = model  # ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ•°åå¤‰ãˆã¦ã„ã‚‹ã ã‘

# ä¾‹æ–‡
tx = 'galveston is home to six [MASK] districts with over 60 structures listed representing architectural significance'
# ãƒˆãƒ¼ã‚¯ãƒ³ãƒŠã‚¤ã‚º
inputs = tokenizer(tx, return_tensors="tf")
logits = mlm_bert(**inputs).logits
mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])
selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)
predicted_token_id = tf.math.argmax(selected_logits, axis=-1)
tokenizer.decode(predicted_token_id)
```
![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/542929/bd50c471-7abd-5bf1-e263-67b4d0d0bcae.png)

ã¡ãªã¿ã«ã€å…¨ãå­¦ç¿’ã•ã›ã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã—ã¦ã¿ã‚‹ã¨ä¸æ­£è§£ã ã£ãŸã®ã§ã¡ã‚ƒã‚“ã¨å­¦ç¿’ã§ãã¦ã„ãŸã‚ˆã†ã ã€‚
```python
config = BertConfig()
model_tmp = TFBertForMaskedLM(config)
logits = model_tmp(**inputs).logits
mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])
selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)
predicted_token_id = tf.math.argmax(selected_logits, axis=-1)
tokenizer.decode(predicted_token_id)
```
![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/542929/e43b35cf-9807-e813-8813-24cf569bf699.png)

# Kerasã‚’ä½¿ã£ãŸBERTã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ–‡æ›¸åˆ†é¡ï¼‰ã®æ–¹æ³•
Kerasã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’æ›¸ãã€‚

ã¾ãšå¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’importã€‚ï¼ˆTPUã§ã¯ãªãã€GPUã‚’ä½¿ã£ãŸã€‚ï¼‰
```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import glob
from tqdm import tqdm
import scipy
import gc
import pickle
import os
import collections
import random
import string
import re
import sklearn
from sklearn import datasets
from sklearn.model_selection import StratifiedKFold

import tensorflow as tf  # 2.13.0
import tensorflow.keras.layers as L
import tensorflow.keras.models as M

import transformers  # 4.35.2
from transformers import AutoTokenizer, TFAutoModel, TFBertForMaskedLM, TFBertForPreTraining
from transformers import BertConfig
from transformers import DataCollatorForLanguageModeling
from transformers import AdamWeightDecay

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')
import spacy
from spacy.lang.en.stop_words import STOP_WORDS

from datasets import load_dataset
print(tf.__version__)
print(transformers.__version__)
# > 2.13.0
# > 4.35.0
```
ãƒ‡ãƒ¼ã‚¿ã¯"20 newsgroups text dataset"ã‚’ä½¿ã†ã€‚è¨˜å·ã¨ã‹æ•°å­—ã¨ã‹ã®å‰å‡¦ç†ã‚’é©ç”¨ã—ã¦ã€Trainã¨Validationãƒ‡ãƒ¼ã‚¿ã‚’åˆ†ã‘ã¦ãŠãã€‚
```python
# å°æ–‡å­—åŒ–ã€è¨˜å·ãƒ»ä¸¸å›²ã„æ•°å­—é™¤å»ã€å…¨è§’ã‚’åŠè§’ãªã©å‰å‡¦ç†é–¢æ•°
def preprocessing(text):
    text = text.lower()  # å°æ–‡å­—åŒ–
    text = re.sub('\r\n','',text)  # \r\nã‚’delete
    text = re.sub('\n',' ',text)  # \r\nã‚’delete
    text = re.sub(r'\d+','0',text)  # æ•°å­—åˆ—ã‚’delete
    ZEN = "".join(chr(0xff01 + i) for i in range(94)).replace('ï¼','').replace('ï¼','').replace('ï¼','').replace('ï¼Ÿ','')  # å…¨è§’æ–‡å­—åˆ—ä¸€è¦§
    HAN = "".join(chr(0x21 + i) for i in range(94)).replace('-','').replace('.','').replace('!','').replace('?','')  # åŠè§’æ–‡å­—åˆ—ä¸€è¦§
    ALL=re.sub(r'[a-zA-Zï½-ï½šï¼¡-ï¼º\d]+','',ZEN+HAN).replace('.','').replace('ï¼','').replace('!','').replace('ï¼','').replace('?','').replace('ï¼Ÿ','')  # ãƒ‰ãƒƒãƒˆã¯æ®‹ã™
    code_regex = re.compile('['+'ã€œ'+'ã€'+'ã€‚'+'~'+'*'+'ï¼Š'+ALL+'ã€Œ'+'ã€'+']')
    text = code_regex.sub('', text)  # è¨˜å·ã‚’æ¶ˆã™

    ZEN = "".join(chr(0xff01 + i) for i in range(94))
    HAN = "".join(chr(0x21 + i) for i in range(94))
    ZEN2HAN = str.maketrans(ZEN, HAN)
    HAN2ZEN = str.maketrans(HAN, ZEN)
    text=text.translate(ZEN2HAN)  # å…¨è§’ã‚’åŠè§’ã«
    text = text.replace('-','')
    text = re.sub('[ ã€€]+', ' ', text)
    text = re.sub('[..]+', '.', text)
    text = re.sub('[\t]+', '', text)
    return text

text_column_name = 'text'
data = datasets.fetch_20newsgroups()
data_valid = pd.DataFrame({text_column_name:data.data, 'label':data.target}).iloc[10000:,:].copy()
data = pd.DataFrame({text_column_name:data.data, 'label':data.target}).iloc[0:10000,:]
data[text_column_name] = data[text_column_name].map(lambda x: preprocessing(x))
data_valid[text_column_name] = data_valid[text_column_name].map(lambda x: preprocessing(x))
print(len(data),len(data_valid))
display(data.head())
```
![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/542929/1cb52bf5-43cf-abfb-3b7e-cbc88adea915.png)

ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®é™¤å»ã€‚
```python
spacy_model = spacy.load('en_core_web_sm')
stopwords_spacy = list(spacy_model.Defaults.stop_words)
stop_words_nltk = (stopwords.words('english'))
stopWords = list(set(stop_words_nltk+stopwords_spacy))
stopWordsDict = {i:'' for i in stopWords+['']}
# tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]
# print('Tokenized corpus without stopwords:', tokenized_corpus_without_stopwords)
def transStopWords(text):
    word_list_noun = [w for w in (text.split(' ')) if w not in stopWords+['']]
    return ' '.join(word_list_noun)

data[text_column_name] = data[text_column_name].map(lambda x: transStopWords(x))
data_valid[text_column_name] = data_valid[text_column_name].map(lambda x: transStopWords(x))
display(data.head())
```
![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/542929/15122db9-e56b-7d31-8696-089821bd17b9.png)

åˆ†é¡ãƒ©ãƒ™ãƒ«ã‚’One-Hotè¡¨ç¾ã«å¤‰ãˆã¦ãŠãå¿…è¦ãŒã‚ã‚‹ã®ã§å¤‰æ›å‡¦ç†ã‚’å®Ÿæ–½ã€‚
```python
num_class = data['label'].unique().shape[0]
data_label = tf.keras.utils.to_categorical(data['label'], num_classes=num_class)
data_valid_label = tf.keras.utils.to_categorical(data_valid['label'], num_classes=num_class)
```

tokenizerã®èª­ã¿è¾¼ã¿ã€‚ä»Šå›ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯DeBERTaã‚’ä½¿ã†ã®ã§ã€DeBERTaã®tokenizerã‚’loadã€‚
```python
max_length = 200
tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')
```

ãƒ‡ãƒ¼ã‚¿ã‚’ã¾ã¨ã‚ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³ãƒŠã‚¤ã‚ºå‡¦ç†ã™ã‚‹ã€‚è¤‡æ•°ã®æ–‡æ›¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ãƒŠã‚¤ã‚ºã™ã‚‹å ´åˆã¯ã€```batch_encode_plus()```ã‚’ä½¿ã†ã€‚ï¼ˆ[huggingface Tokenizer ã® tokenize, encode, encode_plus ãªã©ã®é•ã„](https://zenn.dev/hellorusk/articles/7fd588cae5b173)ï¼‰

```python
train_tokens = tokenizer.batch_encode_plus(
    data[text_column_name].to_list(),
    padding = "max_length",
    max_length = max_length,
    truncation = True, return_tensors='tf', add_special_tokens=True
)

valid_tokens = tokenizer.batch_encode_plus(
    data_valid[text_column_name].to_list(),
    padding = "max_length",
    max_length = max_length,
    truncation = True, return_tensors='tf', add_special_tokens=True
)
```

æ¬¡ã«ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
æ§‹ç¯‰æ–¹æ³•ã¯ã€Œ[TensorFlow(Keras)ã¨BERTã§æ–‡ç« åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹æ–¹æ³•](https://htomblog.com/python-tensorflowbert)ã€ã‚’å‚è€ƒã«ã—ãŸã€‚

```TFAutoModel.from_pretrained('microsoft/deberta-v3-base')```ã§DeBERTaã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’loadã—ã¦ã„ã‚‹ã€‚ï¼ˆPytorchã®å ´åˆ```AutoModel.from_pretrained```ã‚’ä½¿ã†ã€‚ï¼‰

ã€Œ[TensorFlow(Keras)ã¨BERTã§æ–‡ç« åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹æ–¹æ³•](https://htomblog.com/python-tensorflowbert)ã€ã§ã¯ã€pooler_outputã®å‡ºåŠ›ã‚’ä½¿ã£ã¦ã„ã‚‹ãŒã€DeBERTaã¯pooler_outputã®å‡ºåŠ›ã‚’è¿”ã•ãªã„ã®ã§ã€last_hidden_stateã‚’å–ã‚Šå‡ºã—ãŸã€‚
pooler_outputã¨last_hidden_stateã®é•ã„ã¯ã€Œ[[Github] Difference between CLS hidden state and pooled_output #7540](https://github.com/huggingface/transformers/issues/7540)ã€ã«æ›¸ã„ã¦ã„ã‚‹ã€‚

DeBERTaã®äº‹å‰å­¦ç¿’æ¸ˆã¿ã®å±¤ã®å­¦ç¿’ã¯ã—ãªã„ã‚ˆã†ã«ã€```bert_model.trainable = False```ã«ã—ã¦ã„ã‚‹ã€‚
æœ€åˆã€```bert_model.trainable = True```ã§å®Ÿè¡Œã—ãŸã®ã ãŒã€epochãŒé€²ã‚“ã§ã‚‚val_accãŒ0.5ã«å¼µã‚Šä»˜ã„ãŸã¾ã¾å…¨ãå‘ä¸Šã—ãªã„äº‹è±¡ãŒèµ·ãã€è‰²ã€…èª¿ã¹ã¦ã„ã‚‹ã¨ã€Œ[[Github] Transfer learning & fine-tuning](https://github.com/keras-team/keras-io/blob/master/guides/ipynb/transfer_learning.ipynb)ã€ã‚’è¦‹ã¤ã‘ãŸã€‚
ã€Œ[[Github] Transfer learning & fine-tuning](https://github.com/keras-team/keras-io/blob/master/guides/ipynb/transfer_learning.ipynb)ã€ã«ã‚ˆã‚‹ã¨ã€
>è¨³ï¼šãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸå­¦ç¿’å¯èƒ½ãªå±¤ã¨ã€äº‹å‰ã«å­¦ç¿’ã•ã‚ŒãŸç‰¹å¾´ã‚’ä¿æŒã™ã‚‹å­¦ç¿’å¯èƒ½ãªå±¤ãŒæ··åœ¨ã—ã¦ã„ã‚‹å ´åˆã€ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸå±¤ã¯å­¦ç¿’ä¸­ã«éå¸¸ã«å¤§ããªå‹¾é…æ›´æ–°ã‚’å¼•ãèµ·ã“ã—ã€äº‹å‰ã«å­¦ç¿’ã•ã‚ŒãŸç‰¹å¾´ã‚’ç ´å£Šã—ã¦ã—ã¾ã„ã¾ã™ã€‚

ã¨ã„ã†ã“ã¨ã§ã€äº‹å‰å­¦ç¿’ã®ç‰¹å¾´ã‚’ç ´å£Šã—ã¦ã—ã¾ã†ã®ã§ã€BERTã®å±¤ã¯Freezeã•ã›ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸæ–¹ãŒã„ã„ã‚‰ã—ã„ã€‚
BERTã®å±¤ã‚’å†èª¿æ•´ã—ãŸã„å ´åˆã¯ã€1åº¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸå¾Œã€ä½ã„å­¦ç¿’ç‡ã§```bert_model.trainable = True```ã«ã—ã¦ã‚‚ã†1åº¦å­¦ç¿’ã‚’å›ã—ãŸæ–¹ãŒã„ã„ã¨ã®ã“ã¨ã€‚
>è¨³ï¼šãƒ¢ãƒ‡ãƒ«ãŒæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã«åæŸã—ãŸã‚‰ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å…¨éƒ¨ã¾ãŸã¯ä¸€éƒ¨ã®ãƒ•ãƒªãƒ¼ã‚ºã‚’è§£é™¤ã—ã¦ã€éå¸¸ã«ä½ã„å­¦ç¿’ç‡ã§ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’end-to-endã§å†å­¦ç¿’ã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```bert_model.trainable = False```ã«å¤‰æ›´ã—ã¦å­¦ç¿’ã•ã›ã‚‹ã¨ã€epochãŒé€²ã‚€ã¨åŒæ™‚ã«val_accã‚‚å‘ä¸Šã—å§‹ã‚ãŸã®ã§ã€```bert_model.trainable = True```ã§å­¦ç¿’ã—ãŸã¨ãã¯äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã«æ‚ªå½±éŸ¿ãŒã‚ã£ãŸã£ã½ã„ã€‚

ä»Šå›ã¯æ™‚é–“ã‚‚ã‹ã‹ã‚‹ã®ã§ã€å†åº¦```bert_model.trainable = True```ã«ã—ã¦DeBERTaã®äº‹å‰å­¦ç¿’æ¸ˆã¿ã®å±¤ã‚’å†èª¿æ•´ã™ã‚‹ã“ã¨ã¯ã—ãªã„ãŒã€è¦šãˆã¦ãŠããŸã„ã€‚
```python
def get_model(max_length, num_classes):
    # input_idsã‚’å—ã‘å–ã‚‹å±¤
    input_ids = L.Input(
        shape = (max_length), dtype = tf.int32, name = "input_ids"
    )

    # attention_maskã‚’å—ã‘å–ã‚‹å±¤
    attention_mask = L.Input(
        shape = (max_length), dtype = tf.int32, name = "attention_mask"
    )

    # token_type_idsã‚’å—ã‘å–ã‚‹å±¤
    token_type_ids = L.Input(
        shape = (max_length), dtype = tf.int32, name = "token_type_ids"
    )
    
    # BERTãƒ¢ãƒ‡ãƒ«
    bert_model = TFAutoModel.from_pretrained('microsoft/deberta-v3-base')
    bert_model.trainable = False  # æœ€åˆã¯Falseã§å®Ÿæ–½
    transformer_outputs = bert_model(
        {"input_ids": input_ids,
         "attention_mask": attention_mask,
         "token_type_ids": token_type_ids}
    )
    pooler_output = transformer_outputs.last_hidden_state[:,0,:]  # æœ€çµ‚éš ã‚Œå±¤
    #pooler_output = transformer_outputs.pooler_output

    # BERTã®å‡ºåŠ›->ã‚¯ãƒ©ã‚¹æ•°ã«å¤‰æ›ã™ã‚‹å±¤
    outputs = L.Dense(units = num_classes, activation = "softmax")(pooler_output)  # 'sigmoid'

    # å®šç¾©ã—ãŸå±¤ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    model = M.Model(
        inputs = [input_ids, attention_mask, token_type_ids],
        outputs = outputs
    )

    # æœ€é©åŒ–æ‰‹æ³•ã¨æå¤±é–¢æ•°ã‚’å®šç¾©
    model.compile(
        optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),
        loss = 'categorical_crossentropy',  # "binary_crossentropy",
        #metrics=[tf.keras.metrics.AUC()]
        metrics=['acc']
    )
    return model

model = get_model(max_length, num_class)
model.summary()
```
ï¼ˆäº‹å‰å­¦ç¿’æ¸ˆã¿ã®å±¤ã¯å­¦ç¿’ã•ã›ãªã„ï¼‰
![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/542929/e168c13c-b9ca-b1bd-6ff3-bb93687954c6.png)

ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ãŒæ¸ˆã‚“ã ã®ã§ã€å­¦ç¿’ã•ã›ã¦ã„ãã€‚
ã¾ãšãƒˆãƒ¼ã‚¯ãƒ³ãƒŠã‚¤ã‚ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’Tensorflowã®Dataset APIã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŒ–ã™ã‚‹ã€‚
```batch_encode_plus()```ã§ãƒˆãƒ¼ã‚¯ãƒ³ãƒŠã‚¤ã‚ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿(train_tokens)ã«å…¥ã£ã¦ã„ã‚‹"input_ids"ã€"attention_mask"ã€"token_type_ids"ã¨ã€One-Hotè¡¨ç¾ã®æ­£è§£ãƒ©ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿(data_label)ã‚’```tf.data.Dataset.from_tensor_slices()```ã«æ¸¡ã›ã°OKã€‚
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦```.batch(8)```ã®ã‚ˆã†ã«æ›¸ã„ã¦ã‚ã’ã‚‹ã¨ã€ãƒãƒƒãƒåŒ–ã§ãã‚‹ã®ã§ãƒ¢ãƒ‡ãƒ«ã®```model.fit()```æ™‚ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æŒ‡å®šã—ã¦ã‚ã’ã‚‹å¿…è¦ã¯ãªããªã‚‹ã€‚
ã‚ã¨ã¯ã€æ™®æ®µã®ã‚ˆã†ã«```ModelCheckpoint```ã‚„```EarlyStopping```ã‚’æŒ‡å®šã—ã¦ã€å­¦ç¿’ã•ã›ã¦ã‚ã’ã‚Œã°OKã€‚ï¼ˆäº¤å·®æ¤œè¨¼ã®ã‚³ãƒ¼ãƒ‰ã¯ã€Œ[kaggleã§ã‚ˆãä½¿ã†äº¤å·®æ¤œè¨¼ãƒ†ãƒ³ãƒ—ãƒ¬(Keraså‘ã‘)](https://zenn.dev/monda/articles/kaggle-cv-template)ã€ã‚’å‚ç…§ã—ãŸã€‚ï¼‰

```python
%%time
valid_scores = []
models = []
skf = StratifiedKFold(n_splits=3, random_state=0, shuffle=True)
for fold, (train_indices, valid_indices) in enumerate(skf.split(train_tokens['input_ids'].numpy(), data.label.to_numpy())):
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Tensorfloç”¨ã«åŠ å·¥
    train_dataset = tf.data.Dataset.from_tensor_slices((
        {"input_ids": train_tokens["input_ids"].numpy()[train_indices],
         "attention_mask": train_tokens["attention_mask"].numpy()[train_indices],
         "token_type_ids": train_tokens["token_type_ids"].numpy()[train_indices]},
        data_label[train_indices,:]
    ))

    valid_dataset = tf.data.Dataset.from_tensor_slices((
        {"input_ids": train_tokens["input_ids"].numpy()[valid_indices],
         "attention_mask": train_tokens["attention_mask"].numpy()[valid_indices],
         "token_type_ids": train_tokens["token_type_ids"].numpy()[valid_indices]},
        data_label[valid_indices,:]
    ))
    
    valid_dataset2 = tf.data.Dataset.from_tensor_slices((
        {"input_ids": valid_tokens["input_ids"].numpy(),
         "attention_mask": valid_tokens["attention_mask"].numpy(),
         "token_type_ids": valid_tokens["token_type_ids"].numpy()},
        data_valid_label
    ))

    # ãƒãƒƒãƒåŒ–ã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    train_dataset_b = train_dataset.batch(8)
    train_dataset_b = train_dataset_b.shuffle(len(train_indices))
    valid_dataset_b = valid_dataset.batch(8)
    valid_dataset_b = valid_dataset_b.shuffle(len(valid_indices))
    valid_dataset2_b = valid_dataset2.batch(8)
    
    checkpoint = tf.keras.callbacks.ModelCheckpoint(
        #"/kaggle/working/model"+str(fold)+"-{epoch:02d}-{val_auc:.2f}.keras"
        "/kaggle/working/model"+str(fold)+".keras"
        ,monitor = "val_loss",direction = "min"
        ,save_best_only = True
        #,period=2
        ,save_weights_only = True
)

    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
    model.fit(train_dataset_b,
              validation_data=valid_dataset_b,
              epochs=100,
              callbacks=[checkpoint, callback],
              #verbose=0
             )
    
    valid_loss, valid_acc = model.evaluate(valid_dataset2, verbose=0)
    valid_scores.append(valid_acc)

    models.append(model)

cv_score = np.mean(valid_scores)
print(f'CV score: {cv_score}')
```
val_accãŒä¸ŠãŒã£ã¦ã„ã£ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ããŸã€‚ï¼ˆGPUã®åˆ©ç”¨æ™‚é–“ã«åˆ¶é™ãŒã‚ã‚‹ã®ã§é€”ä¸­ã§æ­¢ã‚ãŸã€‚æœ€çµ‚çš„ãªç²¾åº¦ã¯ä¸æ˜ã€‚ï¼‰
![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/542929/6e4a96be-cb6b-8515-0b37-92904e2901ec.png)

# ãŠã‚ã‚Šã«
Kerasã§BERTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸäº‹å‰å­¦ç¿’ã‚„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ãŸã€‚ã“ã‚Œã§Kerasã§transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¯ãƒ©ã‚¹ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã£ãŸæ°—ãŒã™ã‚‹ã€‚ãŸã æœ€è¿‘ã€tf.kerasã§ã¯ãªãã€ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ã®Keras 3.0ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸã®ã§ãã£ã¡ã ã¨ã¾ãŸå¾®å¦™ã«æ›¸ãæ–¹ã¨ã‹ãŒé•ã†å¯èƒ½æ€§ã‚‚ã‚ã‚‹ãªã¨æ€ã£ã¦ã„ã‚‹ã€‚ãã®è¾ºã¯ã¾ãŸè¿½ã€…ãŠå‹‰å¼·ã—ã¾ã™ã‹ã­ã€‚

ä»¥ä¸Šï¼
