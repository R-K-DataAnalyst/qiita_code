{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd89d3-6602-4f98-8922-95f4eddaeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#必要ライブラリのインポート\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.ticker as ptick\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from matplotlib.cm import get_cmap\n",
    "import japanize_matplotlib\n",
    "\n",
    "import seaborn as sns\n",
    "import mpl_toolkits\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import sklearn.metrics\n",
    "from sklearn import preprocessing\n",
    "import rich.table\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "\n",
    "import geopandas as gpd\n",
    "import geoplot as gplt\n",
    "import cartopy, fiona, shapely, pyproj, rtree, pygeos\n",
    "import cv2\n",
    "import rasterio as rio\n",
    "from rasterio import plot\n",
    "from rasterio.plot import show, plotting_extent\n",
    "from rasterio.mask import mask\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.features import geometry_mask, shapes\n",
    "from shapely.geometry import MultiPolygon, Polygon, shape\n",
    "import rasterstats\n",
    "import earthpy.spatial as es\n",
    "import earthpy.plot as ep\n",
    "import libpysal\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import urllib\n",
    "from satsearch import Search\n",
    "from pystac_client import Client\n",
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "from pystac_client import Client\n",
    "import warnings\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pymc as pm\n",
    "import jax\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "import pytensor\n",
    "import gc\n",
    "import optuna.integration.lightgbm as opt_lgb\n",
    "\n",
    "# CPU Multi\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n",
    "print(jax.default_backend())\n",
    "print(jax.devices(\"cpu\"))\n",
    "\n",
    "# 日本語フォント読み込み\n",
    "japanize_matplotlib.japanize()\n",
    "# jpn_fonts=list(np.sort([ttf for ttf in fm.findSystemFonts() if 'ipaexg' in ttf or 'msgothic' in ttf or 'japan' in ttf or 'ipafont' in ttf]))\n",
    "# jpn_font=jpn_fonts[0]\n",
    "jpn_font = japanize_matplotlib.get_font_ttf_path()\n",
    "prop = fm.FontProperties(fname=jpn_font)\n",
    "print(jpn_font)\n",
    "plt.rcParams['font.family'] = prop.get_name() #全体のフォントを設定\n",
    "plt.rcParams['figure.dpi'] = 250\n",
    "\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dee3d5-1201-4a0a-8c61-ced75aadcf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データをダウンロードするかどうか\n",
    "DOWNLOAD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4edbe-ce67-4045-8864-67227430477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存するディレクトリの作成\n",
    "dst_dir = 'content/drive/MyDrive/satelite/s2Bands'  # Google Colabでは'/content~~'が正\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# 座標の基準とするラスターデータ読み込み\n",
    "raster_crs = rio.open(os.path.join(dst_dir,'S2B_54SVE_20180428_0_L2A_B11.tif'))\n",
    "raster_profile = raster_crs.profile\n",
    "\n",
    "# 小地域区分のベクターデータ（from e-Stat）を読み込みcrsをラスターデータに合わせる\n",
    "shape_path = \"content/drive/MyDrive/satelite/ibrakiPolygon/\"  # Google Colabでは'/content~~'が正\n",
    "os.makedirs(shape_path, exist_ok=True)\n",
    "part_in_shape = gpd.read_file(os.path.join(shape_path, \"r2ka08235.shp\"), encoding=\"shift-jis\")[['PREF_NAME', 'CITY_NAME', 'S_NAME', 'AREA', 'PERIMETER', 'JINKO', 'SETAI', 'geometry']]\n",
    "re_shape_tsukuba_mirai_2RasterCrs = part_in_shape.to_crs(raster_profile[\"crs\"])  #crs合わせ\n",
    "print(re_shape_tsukuba_mirai_2RasterCrs.shape)\n",
    "# 同じ小地域がさらに細かく分かれている場合があるので、小地域単位でグループ化しておく\n",
    "re_shape_tsukuba_mirai_2RasterCrs = re_shape_tsukuba_mirai_2RasterCrs.dissolve(['PREF_NAME', 'CITY_NAME', 'S_NAME'], aggfunc='sum', as_index=False)\n",
    "print(re_shape_tsukuba_mirai_2RasterCrs.shape)  # 特定の小地域が統合されレコード数が減る\n",
    "display(re_shape_tsukuba_mirai_2RasterCrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74f1ca-d354-4032-944a-17b103cd8732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 取得範囲を指定するための関数を定義\n",
    "def selSquare(lon, lat, delta_lon, delta_lat):\n",
    "    c1 = [lon + delta_lon, lat + delta_lat]\n",
    "    c2 = [lon + delta_lon, lat - delta_lat]\n",
    "    c3 = [lon - delta_lon, lat - delta_lat]\n",
    "    c4 = [lon - delta_lon, lat + delta_lat]\n",
    "    geometry = {\"type\": \"Polygon\", \"coordinates\": [[ c1, c2, c3, c4, c1 ]]}\n",
    "    return geometry\n",
    "\n",
    "# 茨城県周辺の緯度経度をbbox内へ\n",
    "geometry = selSquare(140.0363, 35.9632, 0.06, 0.02)\n",
    "timeRange = '2018-01-01/2023-12-31' # 取得時間範囲を指定\n",
    "\n",
    "# STACサーバに接続し、取得範囲・時期やクエリを与えて取得するデータを絞る\n",
    "# sentinel:valid_cloud_coverを用いて、雲量の予測をより確からしいもののみに限定している\n",
    "if DOWNLOAD:\n",
    "  api_url = 'https://earth-search.aws.element84.com/v0'\n",
    "  collection = \"sentinel-s2-l2a-cogs\"  # Sentinel-2, Level 2A (BOA)\n",
    "  s2STAC = Client.open(api_url, headers=[])\n",
    "  s2STAC.add_conforms_to(\"ITEM_SEARCH\")\n",
    "\n",
    "  s2Search = s2STAC.search (\n",
    "      intersects = geometry,\n",
    "      datetime = timeRange,\n",
    "      query = {\"eo:cloud_cover\": {\"lt\": 11}, \"sentinel:valid_cloud_cover\": {\"eq\": True}},\n",
    "      collections = collection)\n",
    "\n",
    "  s2_items = [i.to_dict() for i in s2Search.get_items()]\n",
    "  print(f\"{len(s2_items)} のシーンを取得\")\n",
    "\n",
    "# product_idやそのgeometryの情報がまとまったdf作成\n",
    "if DOWNLOAD:\n",
    "  items = s2Search.get_all_items()\n",
    "  df = gpd.GeoDataFrame.from_features(items.to_dict(), crs=\"epsg:32654\")\n",
    "  dfSorted = df.sort_values('eo:cloud_cover').reset_index(drop=True)\n",
    "  # epsgの種類\n",
    "  print('epsg', dfSorted['proj:epsg'].unique())\n",
    "  display(dfSorted.head(3))\n",
    "  # 雲量10以下の日時\n",
    "  print(np.sort(dfSorted[dfSorted['eo:cloud_cover']<=10]['datetime'].unique()))\n",
    "\n",
    "# 2018‐2023各年の同じ時期のproduct_id一覧取得\n",
    "if DOWNLOAD:\n",
    "  df_selected = dfSorted[dfSorted['datetime'].isin(['2018-04-28T01:36:01Z', '2019-05-08T01:37:21Z', '2020-05-02T01:37:21Z', '2021-04-22T01:37:12Z', '2022-04-12T01:37:20Z', '2023-04-27T01:37:18Z'])].copy().sort_values('datetime').iloc[[0,2,3,4,5,6],:]\n",
    "  display(df_selected['sentinel:product_id'].to_list())\n",
    "\n",
    "# 各productのデータURLやtifファイル名の一覧取得\n",
    "if DOWNLOAD:\n",
    "  selected_item = [x.assets for x in items if x.properties['sentinel:product_id'] in (df_selected['sentinel:product_id'].to_list())]\n",
    "  selected_item = sorted(selected_item, key=lambda x:x['thumbnail'].href)\n",
    "\n",
    "# thumbnailで撮影領域確認\n",
    "if DOWNLOAD:\n",
    "  plt.rcParams['font.family'] = prop.get_name() #全体のフォントを設定\n",
    "  fig = plt.figure(figsize=(7,3))\n",
    "  for ix, sitm in enumerate(selected_item):\n",
    "    thumbImg = Image.open(io.BytesIO(requests.get(sitm['thumbnail'].href).content))\n",
    "    ax = plt.subplot(2,3,ix+1)\n",
    "    ax.imshow(thumbImg)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    ax.set_title('撮影日時 : '+'-'.join(sitm['thumbnail'].href.split('/')[-5:-2]), fontsize=4)\n",
    "    ax.grid(False)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "# Sentinel-2のバンド情報を表で示す\n",
    "if DOWNLOAD:\n",
    "  table = rich.table.Table(\"Asset Key\", \"Description\")\n",
    "  for asset_key, asset in selected_item[0].items():\n",
    "    table.add_row(asset_key, asset.title)\n",
    "\n",
    "  display(table)\n",
    "\n",
    "# URLからファイルをダウンロードする関数を定義\n",
    "# 引用：https://note.nkmk.me/python-download-web-images/\n",
    "def download_file(url, dst_path):\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as web_file, open(dst_path, 'wb') as local_file:\n",
    "            local_file.write(web_file.read())\n",
    "    except urllib.error.URLError as e:\n",
    "        print(e)\n",
    "\n",
    "def download_file_to_dir(url, dst_dir):\n",
    "    download_file(url, os.path.join(dst_dir, url.split('/')[-2]+'_'+os.path.basename(url)))\n",
    "\n",
    "\n",
    "# tifファイルをダウンロード(時間かかる)\n",
    "if DOWNLOAD:\n",
    "  # 取得するバンドの選択\n",
    "  bandLists = ['B12','B11','B08','B04','B03','B02'] # SWIR2, SWIR1, NIR, RED, GREEN, BLUE\n",
    "\n",
    "  # 画像のURL取得\n",
    "  file_url = []\n",
    "  for sitm in selected_item:\n",
    "    [file_url.append(sitm[band].href) for band in bandLists if file_url.append(sitm[band].href) is not None]\n",
    "\n",
    "  # 画像のダウンロード\n",
    "  [download_file_to_dir(link, dst_dir) for link in file_url if download_file_to_dir(link, dst_dir) is not None]\n",
    "\n",
    "# ダウンロードファイルリスト（撮影日時順）\n",
    "display(sorted(os.listdir(dst_dir), key=lambda x:(x.split('_54SVE_')[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b493fa-e507-4128-837b-6a5208ce8c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 試しにバンド11のデータを1つ可視化\n",
    "src = rio.open(os.path.join(dst_dir,'S2B_54SVE_20180428_0_L2A_B11.tif'))\n",
    "fig = plt.figure(figsize=(2, 2))\n",
    "ax=plt.subplot(1,1,1)\n",
    "retted = show(src.read(), transform=src.transform, cmap='RdYlGn', ax=ax, vmax=np.quantile(src.read(1), q=0.99))\n",
    "img = retted.get_images()[0]\n",
    "divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', '5%', pad='3%')\n",
    "cbar = plt.colorbar(img, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=4)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "ax.yaxis.offsetText.set_fontsize(4)\n",
    "ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "ax.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# 試しにバンド12のデータを1つ可視化\n",
    "src = rio.open(os.path.join(dst_dir,'S2B_54SVE_20180428_0_L2A_B12.tif'))\n",
    "fig = plt.figure(figsize=(2, 2))\n",
    "ax=plt.subplot(1,1,1)\n",
    "retted = show(src.read(), transform=src.transform, cmap='RdYlGn', ax=ax, vmax=np.quantile(src.read(1), q=0.99))\n",
    "img = retted.get_images()[0]\n",
    "divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', '5%', pad='3%')\n",
    "cbar = plt.colorbar(img, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=4)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "ax.yaxis.offsetText.set_fontsize(4)\n",
    "ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb3825-7a16-4006-8ce8-97580fea33a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラスターデータリスト読み込み\n",
    "getList = sorted(list(glob.glob(dst_dir+'/S2*')), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "display(getList)\n",
    "\n",
    "# ラスターデータをベクターデータの範囲にcropして保存\n",
    "s2Output = 'content/drive/MyDrive/satelite/s2Output'  # Google Colabでは'/content~~'が正\n",
    "os.makedirs(s2Output, exist_ok=True) # outputデータ保存ディレクトリ\n",
    "if DOWNLOAD:\n",
    "  band_paths_list = es.crop_all(list(getList), s2Output, re_shape_tsukuba_mirai_2RasterCrs, overwrite=True)\n",
    "# cropしたラスターデータリスト\n",
    "band_paths_list = sorted(list(glob.glob(s2Output+'/S2*')), key=lambda x:(x.split('_54SVE_')[-1]))  # 撮影日時順にソート\n",
    "display(band_paths_list)\n",
    "\n",
    "# cropしたラスターデータ（つくばみらい市）をベクターデータと共に試しに可視化\n",
    "src = rio.open(band_paths_list[3])  # Band 8\n",
    "fig = plt.figure(figsize=(2, 2))\n",
    "ax=plt.subplot(1,1,1)\n",
    "retted = show(src.read(), transform=src.transform, cmap='coolwarm', ax=ax, vmin=0, vmax=3500)\n",
    "im = retted.get_images()[0]\n",
    "divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", '5%')\n",
    "cbar = fig.colorbar(im, ax=ax, cax=cax, shrink=0.6, extend='both')\n",
    "cbar.ax.tick_params(labelsize=4)\n",
    "re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=1, ax=ax, linewidth=0.2)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "ax.yaxis.offsetText.set_fontsize(4)\n",
    "ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "ax.set_title('_'.join(os.path.basename(src.name).split('_')[-5:-1]), fontsize=4)\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c0b63-592e-4867-a2fa-959c220a20ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各バンドのtifファイルのリスト\n",
    "B04s = sorted(list(glob.glob(os.path.join(s2Output,'S2*B04_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "B08s = sorted(list(glob.glob(os.path.join(s2Output,'S2*B08_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "B11s = sorted(list(glob.glob(os.path.join(s2Output,'S2*B11_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "B12s = sorted(list(glob.glob(os.path.join(s2Output,'S2*B12_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "\n",
    "B03s = sorted(list(glob.glob(os.path.join(s2Output,'S2*B03_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "B02s = sorted(list(glob.glob(os.path.join(s2Output,'S2*B02_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "print(B04s)\n",
    "print(B08s)\n",
    "print(B11s)\n",
    "print(B12s)\n",
    "print(B03s)\n",
    "print(B02s)\n",
    "\n",
    "if DOWNLOAD:\n",
    "    # バンドごとに分解能が違う場合があるので、リサンプリングしてグリッドを合わせる\n",
    "    # 10m分解能のバンドを20m分解能のバンド11のグリッドに合わせる\n",
    "    for num, (b04, b08, b11, b12, b03, b02) in enumerate(zip(B04s, B08s, B11s, B12s, B03s, B02s)):\n",
    "      print('#####', os.path.basename(b08).split('_')[2], '#####')\n",
    "      riod04 = rio.open(b04)\n",
    "      riod08 = rio.open(b08)\n",
    "      riod11 = rio.open(b11)\n",
    "      riod12 = rio.open(b12)\n",
    "      riod03 = rio.open(b03)\n",
    "      riod02 = rio.open(b02)\n",
    "      bounds = riod04.bounds\n",
    "      # print(riod08.read().shape)\n",
    "      # print(riod11.read().shape)\n",
    "      # ラスターデータをリサンプリングしてバンド11に合わせる\n",
    "      riod08_resampling = riod08.read(out_shape=(riod08.count,int(riod11.height),int(riod11.width)),\n",
    "                                      resampling=Resampling.cubic)\n",
    "      riod04_resampling = riod04.read(out_shape=(riod04.count,int(riod11.height),int(riod11.width)),\n",
    "                                      resampling=Resampling.cubic)\n",
    "      riod03_resampling = riod03.read(out_shape=(riod03.count,int(riod11.height),int(riod11.width)),\n",
    "                                      resampling=Resampling.cubic)\n",
    "      riod02_resampling = riod02.read(out_shape=(riod02.count,int(riod11.height),int(riod11.width)),\n",
    "                                      resampling=Resampling.cubic)\n",
    "    \n",
    "      print('B11', riod11.read().shape, riod11.read().shape, sep='-->')\n",
    "      print('B12', riod12.read().shape, riod12.read().shape, sep='-->')\n",
    "      print('B04', riod04.read().shape, riod04_resampling.shape, sep='-->')\n",
    "      print('B08', riod08.read().shape, riod08_resampling.shape, sep='-->')\n",
    "      print('B03', riod03.read().shape, riod03_resampling.shape, sep='-->')\n",
    "      print('B02', riod02.read().shape, riod02_resampling.shape, sep='-->')\n",
    "    \n",
    "      out_meta = riod11.meta\n",
    "      out_meta.update({'dtype':rio.float32})\n",
    "        \n",
    "      fname11 = 'resampling_'+os.path.basename(riod11.name)\n",
    "      with rio.open(os.path.join(s2Output, fname11), \"w\", **out_meta) as dest:\n",
    "        dest.write(riod11.read().astype(rio.float32))\n",
    "    \n",
    "      fname12 = 'resampling_'+os.path.basename(riod12.name)\n",
    "      with rio.open(os.path.join(s2Output, fname12), \"w\", **out_meta) as dest:\n",
    "        dest.write(riod12.read().astype(rio.float32))\n",
    "    \n",
    "      fname08 = 'resampling_'+os.path.basename(riod08.name)\n",
    "      with rio.open(os.path.join(s2Output, fname08), \"w\", **out_meta) as dest:\n",
    "        dest.write(riod08_resampling.astype(rio.float32))\n",
    "    \n",
    "      fname04 = 'resampling_'+os.path.basename(riod04.name)\n",
    "      with rio.open(os.path.join(s2Output, fname04), \"w\", **out_meta) as dest:\n",
    "        dest.write(riod04_resampling.astype(rio.float32))\n",
    "    \n",
    "      fname03 = 'resampling_'+os.path.basename(riod03.name)\n",
    "      with rio.open(os.path.join(s2Output, fname03), \"w\", **out_meta) as dest:\n",
    "        dest.write(riod03_resampling.astype(rio.float32))\n",
    "    \n",
    "      fname02 = 'resampling_'+os.path.basename(riod02.name)\n",
    "      with rio.open(os.path.join(s2Output, fname02), \"w\", **out_meta) as dest:\n",
    "        dest.write(riod02_resampling.astype(rio.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c9dd7-ec64-4849-9d56-f15c1e0cd666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用するディレクトリ\n",
    "dst_dir = 'content/drive/MyDrive/satelite/s2Bands'  # Google Colabでは'/content~~'が正\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "shape_path = \"content/drive/MyDrive/satelite/ibrakiPolygon/\"  # Google Colabでは'/content~~'が正\n",
    "os.makedirs(shape_path, exist_ok=True)\n",
    "\n",
    "s2Output = 'content/drive/MyDrive/satelite/s2Output'  # Google Colabでは'/content~~'が正\n",
    "os.makedirs(s2Output, exist_ok=True) # outputデータ保存ディレクトリ\n",
    "\n",
    "# 座標の基準とするラスターデータ読み込み\n",
    "raster_crs = rio.open(os.path.join(dst_dir,'S2B_54SVE_20180428_0_L2A_B11.tif'))\n",
    "raster_profile = raster_crs.profile\n",
    "\n",
    "# 小地域区分のベクターデータ（from e-Stat）を読み込みcrsをラスターデータに合わせる\n",
    "shape_path = \"content/drive/MyDrive/satelite/ibrakiPolygon/\"  # Google Colabでは'/content~~'が正\n",
    "os.makedirs(shape_path, exist_ok=True)\n",
    "part_in_shape = gpd.read_file(os.path.join(shape_path, \"r2ka08235.shp\"), encoding=\"shift-jis\")[['PREF_NAME', 'CITY_NAME', 'S_NAME', 'AREA', 'PERIMETER', 'JINKO', 'SETAI', 'geometry']]\n",
    "re_shape_tsukuba_mirai_2RasterCrs = part_in_shape.to_crs(raster_profile[\"crs\"])  #crs合わせ\n",
    "print(re_shape_tsukuba_mirai_2RasterCrs.shape)\n",
    "# 同じ小地域がさらに細かく分かれている場合があるので、小地域単位でグループ化しておく\n",
    "re_shape_tsukuba_mirai_2RasterCrs = re_shape_tsukuba_mirai_2RasterCrs.dissolve(['PREF_NAME', 'CITY_NAME', 'S_NAME'], aggfunc='sum', as_index=False)\n",
    "print(re_shape_tsukuba_mirai_2RasterCrs.shape)  # 特定の小地域が統合されレコード数が減る\n",
    "display(re_shape_tsukuba_mirai_2RasterCrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb2177-dc26-4644-b08b-62b9a6587a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 茨城県を含む土壌分類のデータをダウンロードしておく(国土数値情報ダウンロードサイト 土地利用細分メッシュデータ4つ)\n",
    "if DOWNLOAD:\n",
    "    print(1)\n",
    "    ground_truth = gpd.read_file(os.path.join(shape_path, \"L03-b-21_5340-tky.shp\"), encoding=\"shift-jis\")\n",
    "    ground_truth = ground_truth.to_crs(raster_profile[\"crs\"])  #crs合わせ\n",
    "    ground_truth = gpd.overlay(re_shape_tsukuba_mirai_2RasterCrs, ground_truth, how='intersection')  # クロップ\n",
    "    print(2)\n",
    "    ground_truth2 = gpd.read_file(os.path.join(shape_path, \"L03-b-21_5339-tky.shp\"), encoding=\"shift-jis\")\n",
    "    ground_truth2 = ground_truth2.to_crs(raster_profile[\"crs\"])  #crs合わせ\n",
    "    ground_truth2 = gpd.overlay(re_shape_tsukuba_mirai_2RasterCrs, ground_truth2, how='intersection')\n",
    "    print(3)\n",
    "    ground_truth3 = gpd.read_file(os.path.join(shape_path, \"L03-b-21_5440-tky.shp\"), encoding=\"shift-jis\")\n",
    "    ground_truth3 = ground_truth3.to_crs(raster_profile[\"crs\"])  #crs合わせ\n",
    "    ground_truth3 = gpd.overlay(re_shape_tsukuba_mirai_2RasterCrs, ground_truth3, how='intersection')\n",
    "    print(4)\n",
    "    ground_truth4 = gpd.read_file(os.path.join(shape_path, \"L03-b-21_5439-tky.shp\"), encoding=\"shift-jis\")\n",
    "    ground_truth4 = ground_truth4.to_crs(raster_profile[\"crs\"])  #crs合わせ\n",
    "    ground_truth4 = gpd.overlay(re_shape_tsukuba_mirai_2RasterCrs, ground_truth4, how='intersection')\n",
    "\n",
    "# 4つのデータを連結\n",
    "if DOWNLOAD:\n",
    "    ground_truth_2RasterCrs_concat = pd.concat([ground_truth\n",
    "                                                , ground_truth2\n",
    "                                                , ground_truth3\n",
    "                                                , ground_truth4]).reset_index(drop=True)\n",
    "    ground_truth_2RasterCrs_concat.to_file(os.path.join(shape_path, \"ground_truth_2RasterCrs_concat.shp\"), encoding=\"shift-jis\")\n",
    "\n",
    "# 連結した土壌分類でデータ読み込み\n",
    "ground_truth_2RasterCrs_concat = gpd.read_file(os.path.join(shape_path, \"ground_truth_2RasterCrs_concat.shp\"), encoding=\"shift-jis\")\n",
    "display(ground_truth_2RasterCrs_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e4514-fefd-4567-9c11-0775c687ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一部MultiPolygonの地域があるのでPolygonに紐解いておく\n",
    "ground_truth_2RasterCrs_concat_crop_exploded = ground_truth_2RasterCrs_concat.explode().reset_index(drop=True)  # MultiPolygonをPolygonに解く\n",
    "# 土壌分類種をラベルエンコーディングしておく\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "le_L03b_002 = le.fit_transform(ground_truth_2RasterCrs_concat_crop_exploded['L03b_002'])\n",
    "ground_truth_2RasterCrs_concat_crop_exploded['le_L03b_002'] = le_L03b_002\n",
    "display(ground_truth_2RasterCrs_concat_crop_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14155b-f9ee-4dab-8fb0-84f36c317b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 土壌分類のマスタ作成\n",
    "'''\n",
    "https://nlftp.mlit.go.jp/ksj/gml/codelist/LandUseCd-09.html\n",
    "\n",
    "0100\t田\t湿田・乾田・沼田・蓮田及び田とする。\n",
    "0200\tその他の農用地\t麦・陸稲・野菜・草地・芝地・りんご・梨・桃・ブドウ・茶・桐・はぜ・こうぞ・しゅろ等を栽培する土地とする。\n",
    "0300\t-\t-\n",
    "0400\t-\t-\n",
    "0500\t森林\t多年生植物の密生している地域とする。\n",
    "0600\t荒地\tしの地・荒地・がけ・岩・万年雪・湿地・採鉱地等で旧土地利用データが荒地であるところとする。\n",
    "0700\t建物用地\t住宅地・市街地等で建物が密集しているところとする。\n",
    "0800\t-\t-\n",
    "0901\t道路\t道路などで、面的に捉えられるものとする。\n",
    "0902\t鉄道\t鉄道・操車場などで、面的にとらえられるものとする。\n",
    "1000\tその他の用地\t運動競技場、空港、競馬場・野球場・学校・港湾地区・人工造成地の空地等とする。\n",
    "1100\t河川地及び湖沼\t人工湖・自然湖・池・養魚場等で平水時に常に水を湛えているところ及び河川・河川区域の河川敷とする。\n",
    "1200\t-\t-\n",
    "1300\t-\t-\n",
    "1400\t海浜\t海岸に接する砂、れき、岩の区域とする。\n",
    "1500\t海水域\t隠顕岩、干潟、シーパースも海に含める。\n",
    "1600\tゴルフ場\tゴルフ場のゴルフコースの集まっている部分のフェアウエイ及びラフの外側と森林の境目を境界とする。\n",
    "'''\n",
    "area_use_categories = {'0100':'田', '0200':'その他の農用地', '0500':'森林', '0600':'荒地', '0700':'建物用地', '0901':'道路'\n",
    "                       , '0902':'鉄道', '1000':'その他の用地', '1100':'河川地及び湖沼', '1400':'海浜', '1500':'海水域', '1600':'ゴルフ場'\n",
    "                      }\n",
    "area_use_categories_le = {j:area_use_categories[i] for i, j in zip(le.classes_, le.transform(le.classes_))}\n",
    "display(area_use_categories)\n",
    "display(area_use_categories_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e2fce-253c-4bb5-bbc0-a08e3bd794a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 試しにベクターデータを可視化\n",
    "plt.rcParams['font.family'] = prop.get_name() #全体のフォントを設定\n",
    "fig = plt.figure(figsize=(7, 3))\n",
    "ax = plt.subplot(1,3,1)\n",
    "re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='None', ax=ax, edgecolor='k', linewidth=0.3)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "ax.yaxis.offsetText.set_fontsize(4)\n",
    "ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "ax.grid(False)\n",
    "# 一部だけグラウンドトゥルースプロット\n",
    "ax = plt.subplot(1,3,2)\n",
    "re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='None', ax=ax, edgecolor='k', linewidth=0.3)\n",
    "divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', '5%', pad='3%')\n",
    "ground_truth_2RasterCrs_concat_crop_exploded.sample(2000).plot(column='le_L03b_002', facecolor='pink', ax=ax, edgecolor='k', linewidth=0.05, cmap='tab10', legend=True, cax=cax)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "ax.yaxis.offsetText.set_fontsize(4)\n",
    "ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "ax.grid(False)\n",
    "cax.tick_params(labelsize='4')\n",
    "cax.set_yticks([c for c in area_use_categories_le.keys()])\n",
    "cax.set_yticklabels([c for c in area_use_categories_le.values()])\n",
    "# グラウンドトゥルースプロット\n",
    "ax = plt.subplot(1,3,3)\n",
    "re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='None', ax=ax)\n",
    "divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', '5%', pad='3%')\n",
    "img = ground_truth_2RasterCrs_concat_crop_exploded.iloc[:,:].plot(column='le_L03b_002', facecolor='lightpink', ax=ax, edgecolor='k', linewidth=0.05, cmap='tab10', legend=True, cax=cax)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "ax.yaxis.offsetText.set_fontsize(4)\n",
    "ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "ax.grid(False)\n",
    "cax.tick_params(labelsize='4')\n",
    "cax.set_yticks([c for c in area_use_categories_le.keys()])\n",
    "cax.set_yticklabels([c for c in area_use_categories_le.values()])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79444e0c-0d59-43fc-b2b8-f01eb1c63cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各バンドのtifファイルのリスト\n",
    "B04s_resampling = sorted(list(glob.glob(os.path.join(s2Output,'resampling_S2*B04_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "B08s_resampling = sorted(list(glob.glob(os.path.join(s2Output,'resampling_S2*B08_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "B11s_resampling = sorted(list(glob.glob(os.path.join(s2Output,'resampling_S2*B11_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "B12s_resampling = sorted(list(glob.glob(os.path.join(s2Output,'resampling_S2*B12_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "\n",
    "B03s_resampling = sorted(list(glob.glob(os.path.join(s2Output,'resampling_S2*B03_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "B02s_resampling = sorted(list(glob.glob(os.path.join(s2Output,'resampling_S2*B02_crop.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "print(B04s_resampling)\n",
    "print(B08s_resampling)\n",
    "print(B11s_resampling)\n",
    "print(B12s_resampling)\n",
    "print(B03s_resampling)\n",
    "print(B02s_resampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6a947-7dd5-40b7-b742-730f76e6ee40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# バンド演算し、可視化&TIFファイルとして保存\n",
    "fig = plt.figure(figsize=(12, 16))\n",
    "for num, (b04, b08, b11, b12, b03, b02) in enumerate(zip(B04s_resampling, B08s_resampling, B11s_resampling, B12s_resampling, B03s_resampling, B02s_resampling)):\n",
    "    print('#####', os.path.basename(b08).split('_')[3], '#####')\n",
    "    riod04_resampling = rio.open(b04).read()  # Red\n",
    "    riod08_resampling = rio.open(b08).read()  # NIR\n",
    "    riod11 = rio.open(b11)\n",
    "    riod11_resampling = riod11.read()  # SWIR1\n",
    "    riod12 = rio.open(b12)\n",
    "    riod12_resampling = riod12.read()  # SWIR2\n",
    "    riod03_resampling = rio.open(b03).read()  # Green\n",
    "    riod02_resampling = rio.open(b02).read()  # Blue\n",
    "    bounds = riod11.bounds\n",
    "    print(riod04_resampling.shape)\n",
    "    print(riod08_resampling.shape)\n",
    "    print(riod11_resampling.shape)\n",
    "    print(riod12_resampling.shape)\n",
    "    print(riod03_resampling.shape)\n",
    "    print(riod02_resampling.shape)\n",
    "\n",
    "    # 可視近赤外(VNIR)、短波長赤外(SWIR)、熱赤外(TIR)、近赤外(NRI)\n",
    "    # NDBI = SWIR1(Band11)-NIR(Band8) / SWIR1(Band11)+NIR(Band8)  正規化都市化指数\n",
    "    # UI = ((SWIR2 - NIR) / (SWIR2 + NIR))  都市化指数\n",
    "    # NDVI = NIR(Band8) - RED(Band4) / NIR(Band8) + RED(Band4)  正規化植生指数\n",
    "    # GNDVI = ( NIR - Green) / ( NIR + Green)  Green正規化植生指数\n",
    "    # BA = NDBI - NDVI  都市域\n",
    "    # MSAVI2 = (1/2)*(2(NIR+1)-sqrt((2*NIR+1)^2-8(NIR-Red)))  修正土壌調整植生指数\n",
    "      # SAVI(土壌調整係数を使用して、土壌の明るさの影響を最小限に抑えることを目的とした植生指数)での露出土壌の影響を最小限に抑えることを目的とした植生指数\n",
    "    # MNDWI = (Green - SWIR) / (Green + SWIR)  修正正規化水指数\n",
    "    # NDWI = (NIR-SWIR) / (NIR+SWIR)  正規化水指数\n",
    "    # NDSI = (SWIR2 - Green) / (SWIR2 + Green)  正規化土壌指数\n",
    "    # BSI = (SWIR1+Green)-(NIR+Blue) / (SWIR1+Green)+(NIR+Blue)  裸地化指数\n",
    "    # DBSI =( (SWIR1 − GREEN) / (SWIR1 + GREEN) ) − NDVI  乾燥裸地指数\n",
    "    NDBI = ( riod11_resampling.astype(float) - riod08_resampling.astype(float) ) / ( riod11_resampling.astype(float) + riod08_resampling.astype(float) )\n",
    "    UI = ( riod12_resampling.astype(float) - riod08_resampling.astype(float) ) / ( riod12_resampling.astype(float) + riod08_resampling.astype(float) )\n",
    "    NDVI = ( riod08_resampling.astype(float) - riod04_resampling.astype(float) ) / ( riod08_resampling.astype(float) + riod04_resampling.astype(float) )\n",
    "    GNDVI = ( riod08_resampling.astype(float) - riod03_resampling.astype(float) ) / ( riod08_resampling.astype(float) + riod03_resampling.astype(float) )\n",
    "    BA = NDBI - NDVI\n",
    "    MSAVI2 = (1/2)*( 2*(riod08_resampling.astype(float)+1) - np.sqrt( (2*riod08_resampling.astype(float)+1)**2 - 8*(riod08_resampling.astype(float)-riod04_resampling.astype(float)) ) )\n",
    "    MNDWI = (riod03_resampling.astype(float) - riod11_resampling.astype(float)) / (riod03_resampling.astype(float) + riod11_resampling.astype(float))\n",
    "    NDWI = (riod04_resampling.astype(float) - riod11_resampling.astype(float)) / (riod04_resampling.astype(float) + riod11_resampling.astype(float))\n",
    "    NDSI = ( riod12_resampling.astype(float) - riod03_resampling.astype(float) ) / ( riod12_resampling.astype(float) + riod03_resampling.astype(float) )\n",
    "    BSI = ( (riod11_resampling.astype(float) + riod03_resampling.astype(float)) - (riod08_resampling.astype(float) + riod02_resampling.astype(float)) ) / ( (riod11_resampling.astype(float) + riod03_resampling.astype(float)) + (riod08_resampling.astype(float) + riod02_resampling.astype(float)) )\n",
    "    DBSI = ( ( riod11_resampling.astype(float) - riod03_resampling.astype(float) ) / ( riod11_resampling.astype(float) + riod03_resampling.astype(float) ) ) - NDVI\n",
    "    \n",
    "    # NDBI可視化\n",
    "    ax = plt.subplot(11,6,num+1)\n",
    "    img = ax.imshow(NDBI[0], cmap='coolwarm', vmin=np.quantile(NDBI[0], q=0.01), vmax=np.quantile(NDBI[0], q=0.99), extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4)\n",
    "    ax.set_title('NDBI_'+os.path.basename(riod11.name).split('_')[3]+'_'+os.path.basename(riod11.name).split('_')[6], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.5, ax=ax, linewidth=0.5)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "\n",
    "    # UI可視化\n",
    "    ax = plt.subplot(11,6,num+1+6)\n",
    "    img = ax.imshow(UI[0], cmap='coolwarm', vmin=np.quantile(UI[0], q=0.01), vmax=np.quantile(UI[0], q=0.99), extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4)\n",
    "    ax.set_title('UI_'+os.path.basename(riod11.name).split('_')[3]+'_'+os.path.basename(riod11.name).split('_')[6], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.5, ax=ax, linewidth=0.5)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "    \n",
    "    # BA可視化\n",
    "    ax = plt.subplot(11,6,num+1+12)\n",
    "    img = ax.imshow(BA[0], cmap='RdBu_r', vmin=np.quantile(BA[0], q=0.01), vmax=np.quantile(BA[0], q=0.99), extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4)\n",
    "    ax.set_title('BA_'+os.path.basename(riod11.name).split('_')[3]+'_'+os.path.basename(riod11.name).split('_')[6], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.5, ax=ax, linewidth=0.5)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "    \n",
    "    # NDVI可視化\n",
    "    ax = plt.subplot(11,6,num+1+18)\n",
    "    img = ax.imshow(NDVI[0], cmap='RdYlGn', vmin=np.quantile(NDVI[0], q=0.01), vmax=np.quantile(NDVI[0], q=0.99), extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4)\n",
    "    ax.set_title('NDVI_'+os.path.basename(riod11.name).split('_')[3]+'_'+os.path.basename(riod11.name).split('_')[6], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.5, ax=ax, linewidth=0.5)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "\n",
    "    # GNDVI可視化\n",
    "    ax = plt.subplot(11,6,num+1+24)\n",
    "    img = ax.imshow(GNDVI[0], cmap='RdYlGn', vmin=np.quantile(GNDVI[0], q=0.01), vmax=np.quantile(GNDVI[0], q=0.99), extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4)\n",
    "    ax.set_title('GNDVI_'+os.path.basename(riod11.name).split('_')[3]+'_'+os.path.basename(riod11.name).split('_')[6], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.5, ax=ax, linewidth=0.5)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "\n",
    "    # MSAVI2可視化\n",
    "    ax = plt.subplot(11,6,num+1+30)\n",
    "    img = ax.imshow(MSAVI2[0], cmap='RdYlGn', vmin=np.quantile(MSAVI2[0], q=0.01), vmax=np.quantile(MSAVI2[0], q=0.99), extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4)\n",
    "    ax.set_title('MSAVI2_'+os.path.basename(riod11.name).split('_')[3]+'_'+os.path.basename(riod11.name).split('_')[6], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.5, ax=ax, linewidth=0.5)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "\n",
    "    # MNDWI可視化\n",
    "    ax = plt.subplot(11,6,num+1+36)\n",
    "    img = ax.imshow(MNDWI[0], cmap='Blues', vmin=np.quantile(MNDWI[0], q=0.01), vmax=np.quantile(MNDWI[0], q=0.99), extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4)\n",
    "    ax.set_title('MNDWI_'+os.path.basename(riod11.name).split('_')[3]+'_'+os.path.basename(riod11.name).split('_')[6], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.5, ax=ax, linewidth=0.5)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "\n",
    "    # NDWI可視化\n",
    "    ax = plt.subplot(11,6,num+1+42)\n",
    "    img = ax.imshow(NDWI[0], cmap='Blues', vmin=np.quantile(NDWI[0], q=0.01), vmax=np.quantile(NDWI[0], q=0.99), extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4)\n",
    "    ax.set_title('NDWI_'+os.path.basename(riod11.name).split('_')[3]+'_'+os.path.basename(riod11.name).split('_')[6], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.5, ax=ax, linewidth=0.5)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "\n",
    "    # NDSI可視化\n",
    "    ax = plt.subplot(11,6,num+1+48)\n",
    "    img = ax.imshow(NDSI[0], cmap='PuOr_r', vmin=np.quantile(NDSI[0], q=0.01), vmax=np.quantile(NDSI[0], q=0.99), extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4)\n",
    "    ax.set_title('NDSI_'+os.path.basename(riod11.name).split('_')[3]+'_'+os.path.basename(riod11.name).split('_')[6], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.5, ax=ax, linewidth=0.5)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "\n",
    "    # BSI可視化\n",
    "    ax = plt.subplot(11,6,num+1+54)\n",
    "    img = ax.imshow(BSI[0], cmap='PuOr_r', vmin=np.quantile(BSI[0], q=0.01), vmax=np.quantile(BSI[0], q=0.99), extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4)\n",
    "    ax.set_title('BSI_'+os.path.basename(riod11.name).split('_')[3]+'_'+os.path.basename(riod11.name).split('_')[6], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.5, ax=ax, linewidth=0.5)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "\n",
    "    # DBSI可視化\n",
    "    ax = plt.subplot(11,6,num+1+60)\n",
    "    img = ax.imshow(DBSI[0], cmap='PuOr_r', vmin=np.quantile(DBSI[0], q=0.01), vmax=np.quantile(DBSI[0], q=0.99), extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4)\n",
    "    ax.set_title('DBSI_'+os.path.basename(riod11.name).split('_')[3]+'_'+os.path.basename(riod11.name).split('_')[6], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.5, ax=ax, linewidth=0.5)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "    \n",
    "    out_meta = riod11.meta\n",
    "    out_meta.update({'dtype':rio.float32})\n",
    "    fname = 'NDBI_'+os.path.basename(riod11.name)\n",
    "    print(fname)\n",
    "    with rio.open(os.path.join(s2Output, fname), \"w\", **out_meta) as dest:\n",
    "        dest.write(NDBI.astype(rio.float32))\n",
    "\n",
    "    out_meta = riod11.meta\n",
    "    out_meta.update({'dtype':rio.float32})\n",
    "    fname = 'UI_'+os.path.basename(riod11.name)\n",
    "    print(fname)\n",
    "    with rio.open(os.path.join(s2Output, fname), \"w\", **out_meta) as dest:\n",
    "        dest.write(UI.astype(rio.float32))\n",
    "\n",
    "    out_meta = riod11.meta\n",
    "    out_meta.update({'dtype':rio.float32})\n",
    "    fname = 'BA_'+os.path.basename(riod11.name)\n",
    "    print(fname)\n",
    "    with rio.open(os.path.join(s2Output, fname), \"w\", **out_meta) as dest:\n",
    "        dest.write(BA.astype(rio.float32))\n",
    "    \n",
    "    out_meta = riod11.meta\n",
    "    out_meta.update({'dtype':rio.float32})\n",
    "    fname = 'NDVI_'+os.path.basename(riod11.name)\n",
    "    print(fname)\n",
    "    with rio.open(os.path.join(s2Output, fname), \"w\", **out_meta) as dest:\n",
    "        dest.write(NDVI.astype(rio.float32))\n",
    "\n",
    "    out_meta = riod11.meta\n",
    "    out_meta.update({'dtype':rio.float32})\n",
    "    fname = 'GNDVI_'+os.path.basename(riod11.name)\n",
    "    print(fname)\n",
    "    with rio.open(os.path.join(s2Output, fname), \"w\", **out_meta) as dest:\n",
    "        dest.write(GNDVI.astype(rio.float32))\n",
    "\n",
    "    out_meta = riod11.meta\n",
    "    out_meta.update({'dtype':rio.float32})\n",
    "    fname = 'MSAVI2_'+os.path.basename(riod11.name)\n",
    "    print(fname)\n",
    "    with rio.open(os.path.join(s2Output, fname), \"w\", **out_meta) as dest:\n",
    "        dest.write(MSAVI2.astype(rio.float32))\n",
    "\n",
    "    out_meta = riod11.meta\n",
    "    out_meta.update({'dtype':rio.float32})\n",
    "    fname = 'MNDWI_'+os.path.basename(riod11.name)\n",
    "    print(fname)\n",
    "    with rio.open(os.path.join(s2Output, fname), \"w\", **out_meta) as dest:\n",
    "        dest.write(MNDWI.astype(rio.float32))\n",
    "\n",
    "    out_meta = riod11.meta\n",
    "    out_meta.update({'dtype':rio.float32})\n",
    "    fname = 'NDWI_'+os.path.basename(riod11.name)\n",
    "    print(fname)\n",
    "    with rio.open(os.path.join(s2Output, fname), \"w\", **out_meta) as dest:\n",
    "        dest.write(NDWI.astype(rio.float32))\n",
    "\n",
    "    out_meta = riod11.meta\n",
    "    out_meta.update({'dtype':rio.float32})\n",
    "    fname = 'NDSI_'+os.path.basename(riod11.name)\n",
    "    print(fname)\n",
    "    with rio.open(os.path.join(s2Output, fname), \"w\", **out_meta) as dest:\n",
    "        dest.write(NDSI.astype(rio.float32))\n",
    "\n",
    "    out_meta = riod11.meta\n",
    "    out_meta.update({'dtype':rio.float32})\n",
    "    fname = 'BSI_'+os.path.basename(riod11.name)\n",
    "    print(fname)\n",
    "    with rio.open(os.path.join(s2Output, fname), \"w\", **out_meta) as dest:\n",
    "        dest.write(BSI.astype(rio.float32))\n",
    "\n",
    "    out_meta = riod11.meta\n",
    "    out_meta.update({'dtype':rio.float32})\n",
    "    fname = 'DBSI_'+os.path.basename(riod11.name)\n",
    "    print(fname,'\\n')\n",
    "    with rio.open(os.path.join(s2Output, fname), \"w\", **out_meta) as dest:\n",
    "        dest.write(DBSI.astype(rio.float32))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1218f-e964-41a9-923c-3219125bcd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各バンドのtifファイルのリスト\n",
    "NDBI_band_path_list_nomask = sorted(list(glob.glob(os.path.join(s2Output,'NDBI_resampling_S2*.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "UI_band_path_list_nomask = sorted(list(glob.glob(os.path.join(s2Output,'UI_resampling_S2*.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "BA_band_path_list_nomask = sorted(list(glob.glob(os.path.join(s2Output,'BA_resampling_S2*.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "NDVI_band_path_list_nomask = sorted(list(glob.glob(os.path.join(s2Output,'NDVI_resampling_S2*.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "GNDVI_band_path_list_nomask = sorted(list(glob.glob(os.path.join(s2Output,'GNDVI_resampling_S2*.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "MSAVI2_band_path_list_nomask = sorted(list(glob.glob(os.path.join(s2Output,'MSAVI2_resampling_S2*.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "MNDWI_band_path_list_nomask = sorted(list(glob.glob(os.path.join(s2Output,'MNDWI_resampling_S2*.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "NDWI_band_path_list_nomask = sorted(list(glob.glob(os.path.join(s2Output,'NDWI_resampling_S2*.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "NDSI_band_path_list_nomask = sorted(list(glob.glob(os.path.join(s2Output,'NDSI_resampling_S2*.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "BSI_band_path_list_nomask = sorted(list(glob.glob(os.path.join(s2Output,'BSI_resampling_S2*.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "DBSI_band_path_list_nomask = sorted(list(glob.glob(os.path.join(s2Output,'DBSI_resampling_S2*.tif*'))), key=lambda x:(x.split('_54SVE_')[-1]))\n",
    "band_paths_list = NDBI_band_path_list_nomask + UI_band_path_list_nomask + BA_band_path_list_nomask + NDVI_band_path_list_nomask + GNDVI_band_path_list_nomask + MSAVI2_band_path_list_nomask + MNDWI_band_path_list_nomask + NDWI_band_path_list_nomask + NDSI_band_path_list_nomask + BSI_band_path_list_nomask + DBSI_band_path_list_nomask\n",
    "rio_file_list2020 = band_paths_list[2:][::6]\n",
    "display(rio_file_list2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfffb5e-7552-4057-aea2-a9f347ff6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 土壌分類のベクターファイルのPolygon内のバンド演算指標の平均値を計算して、TIFファイルをベクターファイル化\n",
    "def add_stats_vals(rio_data, vec_data, add_col_name, area_col='S_NAME'):\n",
    "    affine = rio_data.transform\n",
    "    mean_stats = rasterstats.zonal_stats(vec_data, rio_data.read(1)\n",
    "                                         , affine=affine\n",
    "                                         , add_stats={'mymean':lambda x: np.ma.mean(x)}) # np.ma.filled(x, fill_value=np.nan)\n",
    "    mean_stats = [m['mymean'] for m in mean_stats]  # 平均値を取得\n",
    "    vec_data_stats = vec_data#.copy()\n",
    "    vec_data_stats[add_col_name] = mean_stats\n",
    "    vec_data_stats[add_col_name] = vec_data_stats[add_col_name].map(lambda x: np.ma.filled(x, fill_value=np.nan))  # Maskの箇所はnanに変更\n",
    "    # 欠損値は小地域ごとの平均値で埋める\n",
    "    category_means = vec_data_stats.dropna(subset=[add_col_name]).groupby(area_col)[add_col_name].mean()  # カテゴリごとの平均を計算\n",
    "    vec_data_stats[add_col_name] = vec_data_stats[add_col_name].fillna(vec_data_stats[area_col].map(category_means))\n",
    "    return vec_data_stats\n",
    "\n",
    "if DOWNLOAD:\n",
    "    ground_truth_2RasterCrs_concat_crop_exploded_stats = ground_truth_2RasterCrs_concat_crop_exploded.copy()\n",
    "    for riofile in rio_file_list2020:\n",
    "        riodata = rio.open(riofile)\n",
    "        ground_truth_2RasterCrs_concat_crop_exploded_stats = add_stats_vals(riodata\n",
    "                                                                            , ground_truth_2RasterCrs_concat_crop_exploded_stats\n",
    "                                                                            , os.path.basename(riofile).split('_')[0]+'_mean'\n",
    "                                                                            , area_col='S_NAME')\n",
    "    ground_truth_2RasterCrs_concat_crop_exploded_stats.to_file(os.path.join(shape_path, \"ground_truth_2RasterCrs_concat_crop_exploded_stats.shp\"), encoding=\"shift-jis\")\n",
    "    display(ground_truth_2RasterCrs_concat_crop_exploded_stats)\n",
    "\n",
    "# SHAPEファイルのカラム名は10文字以内の制限があるので、\"MSAVI2_mean\"は\"MSAVI2_mea\"になってしまっている\n",
    "ground_truth_2RasterCrs_concat_crop_exploded_stats = gpd.read_file(os.path.join(shape_path, \"ground_truth_2RasterCrs_concat_crop_exploded_stats.shp\"), encoding=\"shift-jis\")\n",
    "ground_truth_2RasterCrs_concat_crop_exploded_stats.rename(columns={'MSAVI2_mea':'MSAVI2_mean'}, inplace=True)\n",
    "ground_truth_2RasterCrs_concat_crop_exploded_stats.rename(columns={'le_L03b_00':'le_L03b_002'}, inplace=True)\n",
    "display(ground_truth_2RasterCrs_concat_crop_exploded_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fd349-047c-4af2-bbc5-cc6fde3ec729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# バンド演算のラスターデータplot、バンド演算のベクターデータplot、土壌分類plot\n",
    "fig = plt.figure(figsize=(6, 18))\n",
    "num = 0\n",
    "for i, file in tqdm(enumerate(rio_file_list2020)):\n",
    "    riodata = rio.open(file)\n",
    "    fname = os.path.basename(file)\n",
    "    satIndex = fname.split('_')[0]\n",
    "    if satIndex=='NDBI' or satIndex=='UI' or satIndex=='BA':\n",
    "        cmap = 'coolwarm'\n",
    "    elif satIndex=='NDVI' or satIndex=='GNDVI' or satIndex=='MSAVI2':\n",
    "        cmap = 'RdYlGn'\n",
    "    elif satIndex=='MNDWI' or satIndex=='NDWI':\n",
    "        cmap = 'Blues'\n",
    "    elif satIndex=='NDSI' or satIndex=='BSI' or satIndex=='DBSI':\n",
    "        cmap = 'PuOr_r'\n",
    "    else:\n",
    "        cmap = 'RdYlGn_r'\n",
    "        \n",
    "    ax = plt.subplot(len(rio_file_list2020), 3, num+1)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    retted = show(riodata, ax=ax\n",
    "                  , vmin=np.quantile(riodata.read(), 0.01)\n",
    "                  , vmax=np.quantile(riodata.read(), 0.99)\n",
    "                  , cmap=cmap)\n",
    "    img = retted.get_images()[0]\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=4, width=0.4, length=5)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    ax.set_title(fname.split('_')[0]+'_'+fname.split('_')[4]+'_RasterMap', fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', ax=ax, linewidth=0.2)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "\n",
    "    ax = plt.subplot(len(rio_file_list2020), 3, num+2)\n",
    "    show(riodata, ax=ax\n",
    "         , vmin=np.quantile(riodata.read(), 0.01)\n",
    "         , vmax=np.quantile(riodata.read(), 0.99)\n",
    "         , alpha=0.6, cmap='Greys_r')\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    img = ground_truth_2RasterCrs_concat_crop_exploded_stats.plot(column=satIndex+'_mean', cmap=cmap, edgecolor='k', legend=True, ax=ax, cax=cax\n",
    "                                                                  , linewidth=0.05\n",
    "                                                                  , vmin=np.nanquantile(ground_truth_2RasterCrs_concat_crop_exploded_stats[satIndex+'_mean'], 0.01)\n",
    "                                                                  , vmax=np.nanquantile(ground_truth_2RasterCrs_concat_crop_exploded_stats[satIndex+'_mean'], 0.99)\n",
    "                                                                  #, alpha=0.6\n",
    "                                                                 )\n",
    "    cax.tick_params(labelsize='4', width=0.4, length=5)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    ax.set_title(fname.split('_')[0]+'_'+fname.split('_')[4]+'_VectorMap', fontsize=4)\n",
    "    # re_shape_tsukuba_mirai_2RasterCrs_NDBI.plot(facecolor='none', edgecolor='k', alpha=0.8, ax=ax, linewidth=0.2)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "    \n",
    "    ax = plt.subplot(len(rio_file_list2020), 3, num+3)\n",
    "    show(riodata, ax=ax\n",
    "         , vmin=np.quantile(riodata.read(), 0.01)\n",
    "         , vmax=np.quantile(riodata.read(), 0.99)\n",
    "         , alpha=0.6, cmap='Greys_r')\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    img = ground_truth_2RasterCrs_concat_crop_exploded_stats.plot(column='le_L03b_002', cmap='tab10', edgecolor='k', legend=True, ax=ax, cax=cax\n",
    "                                                                  , linewidth=0.05\n",
    "                                                                  #, vmin=np.nanquantile(ground_truth_2RasterCrs_concat_crop_exploded_stats['NDBI_mean'], 0.01)\n",
    "                                                                  #, vmax=np.nanquantile(ground_truth_2RasterCrs_concat_crop_exploded_stats['NDBI_mean'], 0.99)\n",
    "                                                                  #, alpha=0.6\n",
    "                                                                 )\n",
    "    cax.tick_params(labelsize='4', width=0.4, length=5)\n",
    "    cax.set_yticks([c for c in area_use_categories_le.keys()])\n",
    "    cax.set_yticklabels([c for c in area_use_categories_le.values()])\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    ax.set_title(fname.split('_')[0]+'_'+fname.split('_')[4]+'_SoilCategory', fontsize=4)\n",
    "    # re_shape_tsukuba_mirai_2RasterCrs_NDBI.plot(facecolor='none', edgecolor='k', alpha=0.8, ax=ax, linewidth=0.2)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "    num += 3\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f892d540-2d61-4413-be6e-f0d49c0c633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PolygonをPointにする\n",
    "ground_truth_2RasterCrs_concat_crop_exploded_stats_2point = ground_truth_2RasterCrs_concat_crop_exploded_stats.copy()\n",
    "ground_truth_2RasterCrs_concat_crop_exploded_stats_2point['geometry'] = [p.centroid for p in ground_truth_2RasterCrs_concat_crop_exploded_stats.geometry]\n",
    "\n",
    "# Point可視化（一例としてNDSIのみ）\n",
    "satIndex='NDSI'\n",
    "cmap='PuOr_r'\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "ax = plt.subplot(1,1,1)\n",
    "divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', '5%', pad='3%')\n",
    "img = ground_truth_2RasterCrs_concat_crop_exploded_stats_2point.plot(column=satIndex+'_mean', cmap=cmap, edgecolor='k', legend=True, ax=ax, cax=cax\n",
    "                                                                     , linewidth=0.05, markersize=2\n",
    "                                                                     , vmin=np.quantile(ground_truth_2RasterCrs_concat_crop_exploded_stats[satIndex+'_mean'], 0.01)\n",
    "                                                                     , vmax=np.quantile(ground_truth_2RasterCrs_concat_crop_exploded_stats[satIndex+'_mean'], 0.99)\n",
    "                                                                    )\n",
    "cax.tick_params(labelsize='4', width=0.4, length=5)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "ax.set_title(satIndex, fontsize=4)\n",
    "# re_shape_tsukuba_mirai_2RasterCrs_NDBI.plot(facecolor='none', edgecolor='k', alpha=0.8, ax=ax, linewidth=0.2)\n",
    "ax.yaxis.offsetText.set_fontsize(4)\n",
    "ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfe91b-e7df-4c66-9dc0-1fd219ca4e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 説明変数\n",
    "explanatory_variables = ground_truth_2RasterCrs_concat_crop_exploded_stats_2point.filter(like='mean', axis=1).columns.to_list()\n",
    "# 目的変数\n",
    "objective_variables = 'le_L03b_002'\n",
    "\n",
    "# split data\n",
    "X_train,X_test,y_train,y_test = sklearn.model_selection.train_test_split(ground_truth_2RasterCrs_concat_crop_exploded_stats_2point[explanatory_variables+['geometry']]\n",
    "                                                                         , ground_truth_2RasterCrs_concat_crop_exploded_stats_2point[[objective_variables]+['geometry']]\n",
    "                                                                         , test_size=0.92, shuffle=True, random_state=0\n",
    "                                                                         , stratify=ground_truth_2RasterCrs_concat_crop_exploded_stats_2point[objective_variables])\n",
    "print(X_train.shape, X_test.shape)\n",
    "# 各土壌分類のレコード数\n",
    "display(y_train[objective_variables].value_counts())\n",
    "display(y_test[objective_variables].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345644ca-2e18-45cf-9a3d-c6eee2106746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データとテストデータの土壌分類\n",
    "plt.rcParams['font.family'] = prop.get_name() #全体のフォントを設定\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.subplot(1,2,1)\n",
    "divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', '5%', pad='3%')\n",
    "img = y_train.plot(column=objective_variables, cmap='tab10', edgecolor='k', legend=True, ax=ax, cax=cax, linewidth=0.05, markersize=5)\n",
    "cax.tick_params(labelsize='4', width=0.4, length=5)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "ax.set_title('土壌分類 Point Train', fontsize=4)\n",
    "re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.8, ax=ax, linewidth=0.2)\n",
    "ax.yaxis.offsetText.set_fontsize(4)\n",
    "ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "cax.set_yticks([c for c in area_use_categories_le.keys()])\n",
    "cax.set_yticklabels([c for c in area_use_categories_le.values()])\n",
    "ax.grid(False)\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', '5%', pad='3%')\n",
    "img = y_test.plot(column=objective_variables, cmap='tab10', edgecolor='k', legend=True, ax=ax, cax=cax, linewidth=0.05, markersize=1)\n",
    "cax.tick_params(labelsize='4', width=0.4, length=5)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "ax.set_title('土壌分類 Point Test', fontsize=4)\n",
    "re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.8, ax=ax, linewidth=0.2)\n",
    "ax.yaxis.offsetText.set_fontsize(4)\n",
    "ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "cax.set_yticks([c for c in area_use_categories_le.keys()])\n",
    "cax.set_yticklabels([c for c in area_use_categories_le.values()])\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d678ea-1a5f-475e-a016-ed488c135452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add sample weight\n",
    "def sample_w(y_train):\n",
    "    '''\n",
    "    output sample weight (balanced weight)\n",
    "    y_train:True Train data\n",
    "    '''\n",
    "    n_samples=len(y_train)\n",
    "    n_classes=len(np.unique(y_train))\n",
    "    bincounts = {i:len(y_train[y_train==i]) for i in sorted(np.unique(y_train))}\n",
    "    class_ratio_param = {key:n_samples / (n_classes * bincnt) for key, bincnt in bincounts.items()}\n",
    "    #print('class_ratio_param',class_ratio_param)\n",
    "    sample_weight=np.array([class_ratio_param[r] for r in y_train])\n",
    "    return sample_weight\n",
    "    \n",
    "# 重みの配列\n",
    "sample_ws = sample_w(y_train[objective_variables])\n",
    "\n",
    "# 学習\n",
    "print(y_train[objective_variables].value_counts(),'\\n')\n",
    "lr = sklearn.linear_model.LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train[explanatory_variables], y_train[objective_variables], sample_weight=sample_ws)\n",
    "result_proba = lr.predict_proba(X_train[explanatory_variables])\n",
    "result = lr.predict(X_train[explanatory_variables])\n",
    "reslutDf = pd.DataFrame({'true':y_train[objective_variables].to_numpy(), 'pred':result.ravel()})\n",
    "print(sklearn.metrics.classification_report(y_train[objective_variables].to_numpy(), result.ravel()),'\\n')\n",
    "cm = sklearn.metrics.confusion_matrix(y_train[objective_variables].to_numpy(), result.ravel())\n",
    "print(cm,'\\n\\n######################################\\n')\n",
    "\n",
    "# 未知データに適用\n",
    "print(y_test[objective_variables].value_counts(),'\\n')\n",
    "result_proba = lr.predict_proba(X_test[explanatory_variables])\n",
    "result = lr.predict(X_test[explanatory_variables])\n",
    "reslutDf = pd.DataFrame({'true':y_test[objective_variables].to_numpy(), 'pred':result.ravel()})\n",
    "print(sklearn.metrics.classification_report(y_test[objective_variables].to_numpy(), result.ravel()),'\\n')\n",
    "cm = sklearn.metrics.confusion_matrix(y_test[objective_variables].to_numpy(), result.ravel())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55430be-330e-4f89-b07b-599759971b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # coords(次元やインデックスを定義)\n",
    "    model.add_coord('data', values=range(X_train.shape[0]), mutable=True)\n",
    "    model.add_coord('var', values=explanatory_variables, mutable=True)\n",
    "    model.add_coord('obj_var', values=sorted(y_train[objective_variables].unique()), mutable=True)\n",
    "        \n",
    "    # 説明変数\n",
    "    x = pm.MutableData('x', X_train[explanatory_variables].to_numpy(), dims=('data', 'var'))\n",
    "    y = pm.MutableData(\"y\", y_train[objective_variables].to_numpy(), dims=('data', ))\n",
    "    weights = pm.MutableData(\"weights\", sample_ws, dims=('data', ))\n",
    "    \n",
    "    # 推論パラメータの事前分布\n",
    "    coef_ = pm.Normal('coef', mu=0.0, sigma=1, dims=(\"var\",'obj_var'))  # 各係数の事前分布は正規分布\n",
    "    intercept_ = pm.Normal('intercept', mu=0.0, sigma=1.0, dims=(\"obj_var\", ))  # 切片の事前分布は正規分布\n",
    "    \n",
    "    # linear model\n",
    "    mu = pm.Deterministic(\"mu\", x.dot(coef_)+intercept_, dims=('data', 'obj_var'))\n",
    "    theta = pm.Deterministic(\"theta\", pm.math.softmax(mu, axis=1), dims=('data', 'obj_var'))  # axis設定しないとダメ\n",
    "    \n",
    "    # 尤度\n",
    "    #y_pred = pm.Categorical(\"y_pred\", p=theta, observed=y, dims='data')  # 重み付けしない場合\n",
    "    y_pred = pm.Potential('y_pred', (weights * pm.logp(pm.Categorical.dist(p=theta), y)).sum(axis=0), dims='data')\n",
    "    \n",
    "# モデル構造\n",
    "modeldag = pm.model_to_graphviz(model)\n",
    "display(modeldag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a414b7-2c80-461e-bee2-82ebffbf3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# MCMC実行\n",
    "# バックエンドでNumPyroで実行\n",
    "with model:\n",
    "    # MCMCによる推論\n",
    "    trace_org = pm.sample(draws=3000, tune=1000, chains=3, nuts_sampler=\"numpyro\", random_seed=1, return_inferencedata=True)\n",
    "# >> Wall time: 1min 24s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6981af3-7853-47a9-b267-10e22ea9a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'content/drive/MyDrive/satelite/model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# データの保存 to_netcdfの利用\n",
    "trace_org.to_netcdf(os.path.join(model_dir, 'model_GLM.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd14d91-656d-4e62-8554-3f038120ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'content/drive/MyDrive/satelite/model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# データの読み込み from_netcdfの利用\n",
    "trace_org = az.from_netcdf(os.path.join(model_dir, 'model_GLM.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159e4c7-eafe-47b7-ad53-d1e224e10b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_trace\n",
    "az.plot_trace(trace_org, backend_kwargs={\"constrained_layout\":True}, var_names=[\"coef\",\"intercept\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c588f0-0fc6-44b7-b89a-d3cbcad5f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMCの収束を評価\n",
    "rhat_vals = az.rhat(trace_org).values()\n",
    "\n",
    "# 最大のRhatを確認\n",
    "result = np.max([np.max(i.values) for i in rhat_vals if i.name in ['coef','intercept','mu','theta']])\n",
    "print('Max rhat:', result)\n",
    "# 1.1以上のRhatを確認\n",
    "for i in rhat_vals:\n",
    "    if np.max(i.values)>=1.1:\n",
    "        print(i.name, np.max(i.values), np.mean(i.values), i.values.shape, sep='  ====>  ')\n",
    "# >> Max rhat: 1.0022815343019567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddbfe87-4bac-4211-9cfc-206a7d467a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 各クラスの事後確率theta\n",
    "softmax_result = pd.DataFrame(trace_org['posterior']['theta'].mean(dim=[\"chain\", \"draw\"]).values)\n",
    "y_pred = softmax_result.idxmax(axis=1)\n",
    "print(sklearn.metrics.classification_report(y_train[objective_variables].to_numpy(), y_pred.ravel()))\n",
    "cm = sklearn.metrics.confusion_matrix(y_train[objective_variables].to_numpy(), y_pred.ravel())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbfce3f-ed51-4f4b-8200-929b40de3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未知データへの適用\n",
    "# 回帰係数の推定値\n",
    "coefs = trace_org['posterior']['coef'].mean(dim=[\"chain\", \"draw\"]).values\n",
    "# 切片の推定値\n",
    "intercepts = trace_org['posterior']['intercept'].mean(dim=[\"chain\", \"draw\"]).values\n",
    "# 線形モデル式\n",
    "mu = X_test[explanatory_variables].to_numpy().dot(coefs) + intercepts\n",
    "# ソフトマックス関数に入れる\n",
    "m = softmax(mu, axis=1)\n",
    "# 確率が最大のクラスを取得\n",
    "m_class = m.argmax(axis=1)\n",
    "# 精度計算\n",
    "print(sklearn.metrics.classification_report(y_test[objective_variables].to_numpy(), m_class.ravel()))\n",
    "cm = sklearn.metrics.confusion_matrix(y_test[objective_variables].to_numpy(), m_class.ravel())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0fa1a1-ba41-4f31-89a0-15c0539fbc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最近隣4ゾーンの計算して可視化する\n",
    "# 重心（中心点）の計算\n",
    "coords = X_train.centroid.map(lambda geom: (geom.x, geom.y)).tolist()\n",
    "kd = libpysal.cg.KDTree(np.array(coords))\n",
    "# 最近隣4ゾーンの計算\n",
    "wnn2 = libpysal.weights.KNN(kd, 4)\n",
    "# GeoDataFrameのプロット\n",
    "fig = plt.figure(figsize=(2, 2))\n",
    "ax = plt.subplot(1,1,1)\n",
    "re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', ax=ax, linewidth=0.2)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "ax.set_title('最近隣4ゾーン', fontsize=4)\n",
    "ax.yaxis.offsetText.set_fontsize(4)\n",
    "ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "\n",
    "# 隣接行列のプロット\n",
    "for i, (key, val) in enumerate(wnn2.neighbors.items()):\n",
    "    for neighbor in val:\n",
    "        ax.plot([coords[i][0], coords[neighbor][0]], [coords[i][1], coords[neighbor][1]], color='red', linewidth=0.1\n",
    "                #, marker='+', markersize=0.001, markerfacecolor=\"k\", markeredgecolor=\"k\"\n",
    "               )\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27ac42-e59d-4de1-9f8e-c2718c848da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隣接行列作成\n",
    "adj = wnn2.to_adjlist()\n",
    "adj_symmetry = np.zeros((len(y_train), len(y_train)))\n",
    "for i, (focal, neighbor) in tqdm(enumerate(zip(adj['focal'],adj['neighbor']))):\n",
    "    if (focal, neighbor) in wnn2.asymmetry():\n",
    "        # 非対称の場合、お互いに1とする\n",
    "        adj_symmetry[focal, neighbor] = adj_symmetry[neighbor, focal] = 1\n",
    "        continue\n",
    "    adj_symmetry[focal, neighbor] = adj_symmetry[neighbor, focal] = 1\n",
    "print(adj_symmetry.shape)\n",
    "# >> (851, 851)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf83d06-2d6a-4c28-ab0d-e4c63c51f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 土壌分類コードリスト\n",
    "classK = sorted(y_train[objective_variables].unique())\n",
    "K = len(classK)\n",
    "print(classK)\n",
    "print(K)\n",
    "# >> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# >> 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d609d-f5b1-4f54-a64f-a51b076c0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pm.Model() as model:\n",
    "#     # coords(次元やインデックスを定義)\n",
    "#     model.add_coord('data', values=range(X_train.shape[0]), mutable=True)\n",
    "#     model.add_coord('var', values=explanatory_variables, mutable=True)\n",
    "#     model.add_coord('obj_var', values=sorted(y_train[objective_variables].unique()), mutable=True)\n",
    "    \n",
    "#     # 変数\n",
    "#     x = pm.MutableData('x', X_train[explanatory_variables].to_numpy(), dims=('data', 'var'))\n",
    "#     y = pm.MutableData(\"y\", y_train[objective_variables].to_numpy(), dims=('data', ))\n",
    "#     weights = pm.MutableData(\"weights\", sample_ws, dims=('data', ))\n",
    "#     # pm_adj_symmetry = pm.MutableData(\"adj_symmetry\", adj_symmetry, dims=('data', 'data'))\n",
    "#     print('x shape', x.eval().shape)\n",
    "#     print('y shape', y.eval().shape)\n",
    "#     print('weights shape', weights.eval().shape)\n",
    "#     # print('pm_adj_symmetry shape', pm_adj_symmetry.eval().shape)\n",
    "#     # 分散\n",
    "#     tau = pm.Uniform('tau', lower=0, upper=100, dims=(\"obj_var\", ))\n",
    "#     sigma = pm.Uniform('sigma', lower=0, upper=100)\n",
    "#     print('tau shape', tau.eval().shape)\n",
    "#     print('sigma shape', sigma.eval().shape)\n",
    "    \n",
    "#     # 推論パラメータの事前分布\n",
    "#     coef_ = pm.Normal('coef', mu=0.0, sigma=1, dims=(\"var\",'obj_var'))  # 各係数の事前分布は正規分布\n",
    "#     intercept_ = pm.Normal('intercept', mu=0.0, sigma=1.0, dims=(\"obj_var\", ))  # 切片の事前分布は正規分布\n",
    "#     eps_ = pm.Normal('eps', mu=0.0, sigma=sigma, dims=('data', \"obj_var\"))  # 誤差項\n",
    "#     print('coef shape', coef_.eval().shape)\n",
    "#     print('intercept shape', intercept_.eval().shape)\n",
    "#     print('eps shape', eps_.eval().shape)\n",
    "\n",
    "#     # ICAR\n",
    "#     # 空間相関の項は地域の数分あり、クラス別の差異は分散tauで表現\n",
    "#     ICARs = pm.ICAR('z_car', W=adj_symmetry, sigma=1, dims=('data', ))\n",
    "#     print('ICAR type', ICARs.type)\n",
    "#     # linear model --> 𝑦 = coef_𝑥 + intercept_ + eps_ + ICARs\n",
    "#     mu = pm.Deterministic(\"mu\", x.dot(coef_)+intercept_+(ICARs.reshape((len(y.eval()), 1))*tau)+eps_, dims=('data', 'obj_var'))\n",
    "#     print('x.dot(coef_) shape', x.dot(coef_).eval().shape)\n",
    "#     print('ICARs*tau type', (ICARs*tau).type)\n",
    "#     theta = pm.Deterministic(\"theta\", pm.math.softmax(mu, axis=1), dims=('data', 'obj_var'))  # axis設定しないとダメ\n",
    "#     #y_pred = pm.Categorical(\"y_pred\", p=theta, observed=y, dims='data')\n",
    "#     y_pred = pm.Potential('y_pred', (weights * pm.logp(pm.Categorical.dist(p=theta), y)).sum(axis=0), dims=('data', ))\n",
    "# # モデル構造\n",
    "# modeldag = pm.model_to_graphviz(model)\n",
    "# display(modeldag)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # coords(次元やインデックスを定義)\n",
    "    model.add_coord('data', values=range(X_train.shape[0]), mutable=True)\n",
    "    model.add_coord('var', values=explanatory_variables, mutable=True)\n",
    "    model.add_coord('obj_var', values=sorted(y_train[objective_variables].unique()), mutable=True)\n",
    "    \n",
    "    # 変数\n",
    "    x = pm.MutableData('x', X_train[explanatory_variables].to_numpy(), dims=('data', 'var'))\n",
    "    y = pm.MutableData(\"y\", y_train[objective_variables].to_numpy(), dims=('data', ))\n",
    "    weights = pm.MutableData(\"weights\", sample_ws, dims=('data', ))\n",
    "    # pm_adj_symmetry = pm.MutableData(\"adj_symmetry\", adj_symmetry, dims=('data', 'data'))\n",
    "    print('x shape', x.eval().shape)\n",
    "    print('y shape', y.eval().shape)\n",
    "    print('weights shape', weights.eval().shape)\n",
    "    # print('pm_adj_symmetry shape', pm_adj_symmetry.eval().shape)\n",
    "    # 分散\n",
    "    tau = pm.Uniform('tau', lower=0, upper=100, dims=(\"obj_var\", ))#pm.Exponential('tau', 1)\n",
    "    print('tau shape', tau.eval().shape)\n",
    "    \n",
    "    # 推論パラメータの事前分布\n",
    "    coef_ = pm.Normal('coef', mu=0.0, sigma=1, dims=(\"var\",'obj_var'))  # 各係数の事前分布は正規分布\n",
    "    intercept_ = pm.Normal('intercept', mu=0.0, sigma=1.0, dims=(\"obj_var\", ))  # 切片の事前分布は正規分布\n",
    "    print('coef shape', coef_.eval().shape)\n",
    "    print('intercept shape', intercept_.eval().shape)\n",
    "\n",
    "    # ICAR\n",
    "    # 空間相関の項は地域の数分あり、クラス別の差異は分散tauで表現\n",
    "    ICARs = pm.ICAR('z_car', W=adj_symmetry, sigma=1, dims=('data', ))\n",
    "    print('ICAR type', ICARs.type)\n",
    "    # linear model\n",
    "    mu = pm.Deterministic(\"mu\", x.dot(coef_)+intercept_+(ICARs.reshape((len(y.eval()), 1))*tau), dims=('data', 'obj_var'))\n",
    "    print('x.dot(coef_) shape', x.dot(coef_).eval().shape)\n",
    "    print('ICARs*tau type', (ICARs*tau).type)\n",
    "    theta = pm.Deterministic(\"theta\", pm.math.softmax(mu, axis=1), dims=('data', 'obj_var'))  # axis設定しないとダメ\n",
    "    #y_pred = pm.Categorical(\"y_pred\", p=theta, observed=y, dims='data')\n",
    "    y_pred = pm.Potential('y_pred', (weights * pm.logp(pm.Categorical.dist(p=theta), y)).sum(axis=0), dims=('data', ))\n",
    "# モデル構造\n",
    "modeldag = pm.model_to_graphviz(model)\n",
    "display(modeldag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823f091-99a5-4e2a-bdfa-6865d8c6b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# MCMC実行\n",
    "# バックエンドでNumPyroで実行\n",
    "with model:\n",
    "    # MCMCによる推論\n",
    "    trace = pm.sample(draws=3000, tune=1000, chains=3, nuts_sampler=\"numpyro\", random_seed=1, return_inferencedata=True, idata_kwargs={\"log_likelihood\": False})\n",
    "# >> Wall time: 18min 24s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402a756-e520-427d-8412-31d617167320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "model_dir = 'content/drive/MyDrive/satelite/model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# データの保存 to_netcdfの利用\n",
    "trace.to_netcdf(os.path.join(model_dir, 'model_ICAR.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bd138-4082-4e73-9998-597ab67d732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "model_dir = 'content/drive/MyDrive/satelite/model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# データの読み込み from_netcdfの利用\n",
    "trace = az.from_netcdf(os.path.join(model_dir, 'model_ICAR.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cdec9-7104-491c-8a17-7f92e8801817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_trace\n",
    "az.plot_trace(trace, backend_kwargs={\"constrained_layout\":True}, var_names=[\"coef\",\"intercept\"])#+['z_car'+str(i) for i in range(1,11)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3734630-3229-4ed2-88ac-1f27b75b490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# MCMCの収束を評価\n",
    "rhat_vals = az.rhat(trace).values()\n",
    "\n",
    "# 最大のRhatを確認\n",
    "result = np.max([np.max(i.values) for i in rhat_vals])# if i.name in [\"coef\",\"intercept\",'z_car','mu','theta','tau']])\n",
    "print('Max rhat:', result)\n",
    "# 1.1以上のRhatを確認\n",
    "for i in rhat_vals:\n",
    "    if np.max(i.values)>=1.1:\n",
    "        print(i.name, np.max(i.values), np.mean(i.values), i.values.shape, sep='  ====>  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33899ea8-99f6-4a42-b780-2f008fe89e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各クラスの事後確率theta\n",
    "softmax_result = pd.DataFrame(trace['posterior']['theta'].mean(dim=[\"chain\", \"draw\"]).values)\n",
    "y_pred = softmax_result.idxmax(axis=1)\n",
    "print(sklearn.metrics.classification_report(y_train[objective_variables].to_numpy(), y_pred.ravel()))\n",
    "cm = sklearn.metrics.confusion_matrix(y_train[objective_variables].to_numpy(), y_pred.ravel())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ca2ad-3ce2-4d4d-b362-7c869544a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データ、テストデータの座標を取得\n",
    "train_coord = pd.DataFrame({'x':X_train['geometry'].x.to_list(), 'y':X_train['geometry'].y.to_list()})\n",
    "test_coord = pd.DataFrame({'x':X_test['geometry'].x.to_list(), 'y':X_test['geometry'].y.to_list()})\n",
    "# 学習データでknn作成\n",
    "knn = sklearn.neighbors.NearestNeighbors(n_neighbors=5)\n",
    "knn.fit(train_coord)\n",
    "# テストデータの各サンプルと最も近い学習データのindexを取得\n",
    "distances, indices = knn.kneighbors(test_coord)\n",
    "test_coord['indices'] = indices[:,0]\n",
    "display(test_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e6e17-27be-48ad-9921-de6fbf9eec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータと最も近い学習データの空間相関変数を取得する\n",
    "z_cars_arr = trace['posterior']['z_car'].mean(dim=[\"chain\", \"draw\"]).values.reshape(-1,1)\n",
    "z_cars_arr_test = z_cars_arr[list(indices[:,0]),:]  # テストデータと最も近い学習データの空間相関を各テストデータに対して計算\n",
    "tau_arr = trace['posterior']['tau'].mean(dim=[\"chain\", \"draw\"]).values\n",
    "z_cars_arr_test = z_cars_arr_test * tau_arr  # 分散をかける\n",
    "print(z_cars_arr_test.shape)\n",
    "# >> (9794, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c104a6-dd69-4dfc-9e2f-2952e1e0e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未知データへの適用\n",
    "# 回帰係数の推定値\n",
    "coefs = trace['posterior']['coef'].mean(dim=[\"chain\", \"draw\"]).values\n",
    "# 切片の推定値\n",
    "intercepts = trace['posterior']['intercept'].mean(dim=[\"chain\", \"draw\"]).values\n",
    "# 線形モデル式\n",
    "mu = X_test[explanatory_variables].to_numpy().dot(coefs) + intercepts + z_cars_arr_test  # 回帰式（係数と切片）と空間相関変数\n",
    "# ソフトマックス関数に入れる\n",
    "m = softmax(mu, axis=1)  # ソフトマックス関数に入れて一般化線形モデルへ\n",
    "# 確率が最大のクラスを取得\n",
    "m_class = m.argmax(axis=1)  # 最も確率が高いクラスを所属クラスとする\n",
    "# 精度計算\n",
    "print(sklearn.metrics.classification_report(y_test[objective_variables].to_numpy(), m_class.ravel()))\n",
    "cm = sklearn.metrics.confusion_matrix(y_test[objective_variables].to_numpy(), m_class.ravel())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee8185-d79e-4212-8763-87e6a90875f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実測データ、予測データ、分類成功可否を可視化\n",
    "def point_plot(results, re_shape_tsukuba_mirai_2RasterCrs, linewidth=0.05, markersize=5, title='', vmin=-8, vmax=8):\n",
    "    plt.rcParams['font.family'] = prop.get_name() #全体のフォントを設定\n",
    "    fig = plt.figure(figsize=(9, 3))\n",
    "    ax = plt.subplot(1,3,1)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    img = results.plot(column=objective_variables, cmap='tab10', edgecolor='k', legend=True, ax=ax, cax=cax, linewidth=linewidth, markersize=markersize)\n",
    "    cax.tick_params(labelsize='4', width=0.4, length=5)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    ax.set_title(title+' Point True', fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.8, ax=ax, linewidth=0.2)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "    cax.set_yticks([c for c in area_use_categories_le.keys()])\n",
    "    cax.set_yticklabels([c for c in area_use_categories_le.values()])\n",
    "    \n",
    "    ax = plt.subplot(1,3,2)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    img = results.plot(column='pred', cmap='tab10', edgecolor='k', legend=True, ax=ax, cax=cax, linewidth=linewidth, markersize=markersize)\n",
    "    cax.tick_params(labelsize='4', width=0.4, length=5)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    ax.set_title(title+' Point Pred', fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.8, ax=ax, linewidth=0.2)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "    cax.set_yticks([c for c in area_use_categories_le.keys()])\n",
    "    cax.set_yticklabels([c for c in area_use_categories_le.values()])\n",
    "    \n",
    "    ax = plt.subplot(1,3,3)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    img = results.plot(column='diff', cmap='binary', edgecolor='k', legend=True, ax=ax, cax=cax, linewidth=linewidth, markersize=markersize, vmin=vmin, vmax=vmax)\n",
    "    cax.tick_params(labelsize='4', width=0.4, length=5)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    ax.set_title(title+' Point False:1, True:0', fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.8, ax=ax, linewidth=0.2)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40acd72e-4bba-40a5-8957-5fd5565e5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化\n",
    "# 学習データ\n",
    "results = y_train.copy()\n",
    "results['pred'] = y_pred.ravel()\n",
    "results['diff'] = results[objective_variables]-results['pred']\n",
    "results.loc[(results['diff']==0), 'diff'] = 0\n",
    "results.loc[(results['diff']!=0), 'diff'] = 1\n",
    "point_plot(results, re_shape_tsukuba_mirai_2RasterCrs, linewidth=0.05, markersize=5, title='土壌分類', vmin=0, vmax=1)\n",
    "\n",
    "# テストデータ\n",
    "results = y_test.copy()\n",
    "results['pred'] = m_class.ravel()\n",
    "results['diff'] = results[objective_variables]-results['pred']\n",
    "results.loc[(results['diff']==0), 'diff'] = 0\n",
    "results.loc[(results['diff']!=0), 'diff'] = 1\n",
    "point_plot(results, re_shape_tsukuba_mirai_2RasterCrs, linewidth=0.05, markersize=1, title='土壌分類', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90554da6-613b-497b-ab89-3ec9a5ade2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b26cd-10a4-484d-a4b9-4cf8f3ec5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:    \n",
    "    # coords(次元やインデックスを定義)\n",
    "    model.add_coord('data', values=range(X_train.shape[0]), mutable=True)\n",
    "    model.add_coord('var', values=explanatory_variables, mutable=True)\n",
    "    model.add_coord('obj_var', values=sorted(y_train[objective_variables].unique()), mutable=True)\n",
    "    \n",
    "    # 変数\n",
    "    x = pm.MutableData('x', X_train[explanatory_variables].to_numpy(), dims=('data', 'var'))\n",
    "    y = pm.MutableData(\"y\", y_train[objective_variables].to_numpy(), dims=('data', ))\n",
    "    weights = pm.MutableData(\"weights\", sample_ws, dims=('data', ))\n",
    "    print('x shape', x.eval().shape)\n",
    "    print('y shape', y.eval().shape)\n",
    "    print('weights shape', weights.eval().shape)\n",
    "\n",
    "    # 分散\n",
    "    tau = pm.Uniform('tau', lower=0, upper=100)#pm.Exponential('tau', 1)\n",
    "    print('tau shape', tau.eval().shape)\n",
    "    \n",
    "    # 推論パラメータの事前分布\n",
    "    coef_ = pm.Normal('coef', mu=0.0, sigma=1, dims=(\"var\",'obj_var'))  # 各係数の事前分布は正規分布\n",
    "    intercept_ = pm.Normal('intercept', mu=0.0, sigma=1.0, dims=(\"obj_var\", ))  # 切片の事前分布は正規分布\n",
    "    print('coef shape', coef_.eval().shape)\n",
    "    print('intercept shape', intercept_.eval().shape)\n",
    "    \n",
    "    # ICARを10個入ったリストを作ってconcatenateしてdata数×class数の空間相関変数としている\n",
    "    ICAR10s = [pm.ICAR('z_car'+str(i+1), W=adj_symmetry, sigma=1, dims=('data', )).reshape((len(y.eval()), 1)) for i, k in enumerate(classK)]\n",
    "    ICAR10s = pm.math.concatenate(ICAR10s, axis=1)\n",
    "\n",
    "    print('ICAR10s type', ICAR10s.type)\n",
    "    # linear model --> 𝑦 = coef_𝑥 + intercept_ + eps_ + ICARs\n",
    "    mu = pm.Deterministic(\"mu\", x.dot(coef_)+intercept_+(ICAR10s*tau), dims=('data', 'obj_var'))\n",
    "    print('x.dot(coef_) shape', x.dot(coef_).eval().shape)\n",
    "    print('ICAR10s*tau type', (ICAR10s*tau).type)\n",
    "    theta = pm.Deterministic(\"theta\", pm.math.softmax(mu, axis=1), dims=('data', 'obj_var'))  # axis設定しないとダメ\n",
    "    #y_pred = pm.Categorical(\"y_pred\", p=theta, observed=y, dims='data')\n",
    "    y_pred = pm.Potential('y_pred', (weights * pm.logp(pm.Categorical.dist(p=theta), y)).sum(axis=0), dims=('data', ))\n",
    "    \n",
    "# モデル構造\n",
    "modeldag = pm.model_to_graphviz(model)\n",
    "display(modeldag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd506bc-26a5-4fd5-a694-08fafaa57fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# MCMC実行\n",
    "# バックエンドでNumPyroで実行\n",
    "with model:\n",
    "    # MCMCによる推論\n",
    "    trace = pm.sample(draws=3000, tune=1000, chains=3, nuts_sampler=\"numpyro\", random_seed=1, return_inferencedata=True, idata_kwargs={\"log_likelihood\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30942ba7-f1ac-46ea-bdc3-c8204394651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "model_dir = 'content/drive/MyDrive/satelite/model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# データの保存 to_netcdfの利用\n",
    "trace.to_netcdf(os.path.join(model_dir, 'model_ICAR10.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084d535-3362-488d-b13e-15b6382daa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "model_dir = 'content/drive/MyDrive/satelite/model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# データの読み込み from_netcdfの利用\n",
    "trace = az.from_netcdf(os.path.join(model_dir, 'model_ICAR10.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39734f9d-7707-4b15-b2a0-7ff63f9a3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_trace\n",
    "az.plot_trace(trace, backend_kwargs={\"constrained_layout\":True}, var_names=[\"coef\",\"intercept\"])#+['z_car'+str(i) for i in range(1,11)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77107908-b03b-478c-9d6e-bee1d0adfcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMCの収束を評価\n",
    "rhat_vals = az.rhat(trace).values()\n",
    "\n",
    "# 最大のRhatを確認\n",
    "result = np.max([np.max(i.values) for i in rhat_vals])# if i.name in [\"coef\",\"intercept\",'z_car','mu','theta','tau']])\n",
    "print('Max rhat:', result)\n",
    "# 1.1以上のRhatを確認\n",
    "for i in rhat_vals:\n",
    "    if np.max(i.values)>=1.1:\n",
    "        print(i.name, np.max(i.values), np.mean(i.values), i.values.shape, sep='  ====>  ')\n",
    "# >> Max rhat: 1.0112367426367925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439c91c-65db-49f8-8f00-d2f3a8d58ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各クラスの事後確率theta\n",
    "softmax_result = pd.DataFrame(trace['posterior']['theta'].mean(dim=[\"chain\", \"draw\"]).values)\n",
    "y_pred = softmax_result.idxmax(axis=1)\n",
    "print(sklearn.metrics.classification_report(y_train[objective_variables].to_numpy(), y_pred.ravel()))\n",
    "cm = sklearn.metrics.confusion_matrix(y_train[objective_variables].to_numpy(), y_pred.ravel())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64eee9-11f8-46f0-ad96-ce125f26b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データ、テストデータの座標を取得\n",
    "train_coord = pd.DataFrame({'x':X_train['geometry'].x.to_list(), 'y':X_train['geometry'].y.to_list()})\n",
    "test_coord = pd.DataFrame({'x':X_test['geometry'].x.to_list(), 'y':X_test['geometry'].y.to_list()})\n",
    "# 学習データでknn作成\n",
    "knn = sklearn.neighbors.NearestNeighbors(n_neighbors=5)\n",
    "knn.fit(train_coord)\n",
    "# テストデータの各サンプルと最も近い学習データのindexを取得\n",
    "distances, indices = knn.kneighbors(test_coord)\n",
    "test_coord['indices'] = indices[:,0]\n",
    "display(test_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dffaf66-31eb-445c-8bc3-3ef923132001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータと最も近い学習データの空間相関変数を取得する\n",
    "z_cars_arr = np.concatenate([trace['posterior']['z_car'+str(i+1)].mean(dim=[\"chain\", \"draw\"]).values.reshape(-1,1) for i, k in enumerate(classK)], axis=1)  # 学習データの空間相関変数の計算\n",
    "z_cars_arr_test = z_cars_arr[list(indices[:,0]),:]  # テストデータと最も近い学習データの空間相関を各テストデータに対して計算\n",
    "tau_arr = trace['posterior']['tau'].mean(dim=[\"chain\", \"draw\"]).values\n",
    "z_cars_arr_test = z_cars_arr_test * tau_arr  # 分散をかける\n",
    "print(z_cars_arr_test.shape)\n",
    "# >> (9794, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e812c-8c08-4dd4-92c7-fe2691ccd5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未知データへの適用\n",
    "# 回帰係数の推定値\n",
    "coefs = trace['posterior']['coef'].mean(dim=[\"chain\", \"draw\"]).values\n",
    "# 切片の推定値\n",
    "intercepts = trace['posterior']['intercept'].mean(dim=[\"chain\", \"draw\"]).values\n",
    "# 線形モデル式\n",
    "mu = X_test[explanatory_variables].to_numpy().dot(coefs) + intercepts + z_cars_arr_test  # 回帰式（係数と切片）と空間相関変数\n",
    "# ソフトマックス関数に入れる\n",
    "m = softmax(mu, axis=1)  # ソフトマックス関数に入れて一般化線形モデルへ\n",
    "# 確率が最大のクラスを取得\n",
    "m_class = m.argmax(axis=1)  # 最も確率が高いクラスを所属クラスとする\n",
    "# 精度計算\n",
    "print(sklearn.metrics.classification_report(y_test[objective_variables].to_numpy(), m_class.ravel()))\n",
    "cm = sklearn.metrics.confusion_matrix(y_test[objective_variables].to_numpy(), m_class.ravel())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a6f21-634e-48ac-b7bf-7260248a5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化\n",
    "# 学習データ\n",
    "results = y_train.copy()\n",
    "results['pred'] = y_pred.ravel()\n",
    "results['diff'] = results[objective_variables]-results['pred']\n",
    "results.loc[(results['diff']==0), 'diff'] = 0\n",
    "results.loc[(results['diff']!=0), 'diff'] = 1\n",
    "point_plot(results, re_shape_tsukuba_mirai_2RasterCrs, linewidth=0.05, markersize=5, title='土壌分類', vmin=0, vmax=1)\n",
    "\n",
    "# テストデータ\n",
    "results = y_test.copy()\n",
    "results['pred'] = m_class.ravel()\n",
    "results['diff'] = results[objective_variables]-results['pred']\n",
    "results.loc[(results['diff']==0), 'diff'] = 0\n",
    "results.loc[(results['diff']!=0), 'diff'] = 1\n",
    "point_plot(results, re_shape_tsukuba_mirai_2RasterCrs, linewidth=0.05, markersize=1, title='土壌分類', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5e0e2-2e5b-42ba-9f8a-e8641b9047df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# バイオリンプロット\n",
    "png_path = 'content/drive/MyDrive/satelite/png/'\n",
    "os.makedirs(png_path, exist_ok=True)\n",
    "figsize=(8,6)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "fig=plt.figure(figsize=figsize)\n",
    "for k, node in enumerate(classK):\n",
    "    softmax1_org = trace_org['posterior']['coef'].mean(dim=[\"chain\"]).values[:,:,k]\n",
    "    softmax1 = trace['posterior']['coef'].mean(dim=[\"chain\"]).values[:,:,k]\n",
    "    corf_df_org = pd.DataFrame(softmax1_org)\n",
    "    corf_df_org.columns=explanatory_variables\n",
    "    corf_df_org['intercept'] = trace_org['posterior']['intercept'].mean(dim=[\"chain\"]).values[:,k]\n",
    "    corf_df_org['tau'] = np.nan\n",
    "    corf_df_org_stack = corf_df_org.stack().reset_index()\n",
    "    corf_df_org_stack['space_corr'] = 0\n",
    "    corf_df_org_stack.columns=['id','col_name','coef','space_corr']\n",
    "    corf_df = pd.DataFrame(softmax1)\n",
    "    corf_df.columns=explanatory_variables\n",
    "    corf_df['intercept'] = trace['posterior']['intercept'].mean(dim=[\"chain\"]).values[:,k]\n",
    "    corf_df['tau'] = trace['posterior']['tau'].mean(dim=[\"chain\"]).values\n",
    "    corf_df_stack = corf_df.stack().reset_index()\n",
    "    corf_df_stack['space_corr'] = 1\n",
    "    corf_df_stack.columns=['id','col_name','coef','space_corr']\n",
    "    corf_df_stack = pd.concat([corf_df_org_stack, corf_df_stack]).reset_index(drop=True)\n",
    "    \n",
    "    ax1 = plt.subplot(3,4,k+1)\n",
    "    img = sns.violinplot(data=corf_df_stack, x='col_name', y='coef', hue=\"space_corr\", split=True, inner=\"quart\", density_norm='width', ax=ax1, linewidth=0.2)\n",
    "    torf_orgs = {}\n",
    "    torfs = {}\n",
    "    for i, col in enumerate(corf_df.columns):\n",
    "        if col=='tau':\n",
    "            ax1.text(ax1.get_xticks()[i], corf_df[col].mean(), round(corf_df[col].mean(),2), fontsize=3, ha=\"left\", color=\"k\")\n",
    "            ax1.text(ax1.get_xticks()[i], corf_df[col].quantile(0.95), round(corf_df[col].quantile(0.95),2), fontsize=2, ha=\"left\", color=\"k\")\n",
    "            ax1.text(ax1.get_xticks()[i], corf_df[col].quantile(0.05), round(corf_df[col].quantile(0.05),2), fontsize=2, ha=\"left\", color=\"k\")\n",
    "        else:\n",
    "            ax1.text(ax1.get_xticks()[i], corf_df_org[col].mean(), round(corf_df_org[col].mean(),2), fontsize=3, ha=\"right\", color=\"k\")\n",
    "            ax1.text(ax1.get_xticks()[i], corf_df[col].mean(), round(corf_df[col].mean(),2), fontsize=3, ha=\"left\", color=\"k\")\n",
    "\n",
    "            ax1.text(ax1.get_xticks()[i], corf_df_org[col].quantile(0.95), round(corf_df_org[col].quantile(0.95),2), fontsize=2, ha=\"right\", color=\"k\")\n",
    "            ax1.text(ax1.get_xticks()[i], corf_df[col].quantile(0.95), round(corf_df[col].quantile(0.95),2), fontsize=2, ha=\"left\", color=\"k\")\n",
    "\n",
    "            ax1.text(ax1.get_xticks()[i], corf_df_org[col].quantile(0.05), round(corf_df_org[col].quantile(0.05),2), fontsize=2, ha=\"right\", color=\"k\")\n",
    "            ax1.text(ax1.get_xticks()[i], corf_df[col].quantile(0.05), round(corf_df[col].quantile(0.05),2), fontsize=2, ha=\"left\", color=\"k\")\n",
    "\n",
    "            if col!='intercept':\n",
    "                torf_orgs[col] = (corf_df_org[col].quantile(0.95)>=0 and corf_df_org[col].quantile(0.05)<=0)\n",
    "                torfs[col] = (corf_df[col].quantile(0.95)>=0 and corf_df[col].quantile(0.05)<=0)\n",
    "\n",
    "    significant_orgs = len([key for key, v in torf_orgs.items() if not v])\n",
    "    significant = len([key for key, v in torfs.items() if not v])\n",
    "    ax1.set_xlabel(None)\n",
    "    ax1.set_ylabel(None)\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=30, fontsize=4, ha='right')\n",
    "    plt.setp(ax1.get_yticklabels(), fontsize=4)\n",
    "    ax1.set_title(f'Class {node}'+' ('+area_use_categories_le[k]+r') coef & $\\tau^{2}$, significant:'+str(significant_orgs)+' and '+str(significant), fontsize=4)\n",
    "    # plt.suptitle(f'Class {node}'+' ('+area_use_categories_le[k]+r') coefficient & $\\tau^{2}$', y=0.90, fontsize=4)\n",
    "    handler, label = ax1.get_legend_handles_labels()\n",
    "    ax1.legend(handler, [\"GLM\",\"ICAR\"], fontsize=4, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure().clear()\n",
    "plt.close()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "del corf_df_org, corf_df_org_stack, corf_df, corf_df_stack, fig\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc086e1-35b9-4b07-8661-f27817886407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 学習データの空間相関変数を可視化\n",
    "png_path = 'content/drive/MyDrive/satelite/png/'\n",
    "os.makedirs(png_path, exist_ok=True)\n",
    "figsize=(8,6)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "fig=plt.figure(figsize=figsize)\n",
    "for k, node in enumerate(['z_car'+str(i+1) for i in classK]):\n",
    "    plot_icar = ground_truth_2RasterCrs_concat_crop_exploded_stats_2point.loc[X_train.index,:].copy()\n",
    "    icar = trace['posterior'][node].mean(dim=[\"chain\",\"draw\"]).values\n",
    "    plot_icar[node] = icar\n",
    "    ax = plt.subplot(3,4,k+1)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    img = plot_icar.plot(column=node, cmap='RdYlBu_r', edgecolor='k', legend=True, ax=ax, cax=cax, linewidth=0.05, markersize=5)\n",
    "    cax.tick_params(labelsize='4', width=0.4, length=5)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    ax.set_title(node+' '+area_use_categories_le[k], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.8, ax=ax, linewidth=0.2)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure().clear()\n",
    "plt.close()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "del plot_icar, fig\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c47d34-2c0b-4841-b358-320ad79c3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの空間相関変数（最も近い学習データの空間相関変数を取得した結果）を可視化\n",
    "png_path = 'content/drive/MyDrive/satelite/png/'\n",
    "os.makedirs(png_path, exist_ok=True)\n",
    "figsize=(8,6)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "fig=plt.figure(figsize=figsize)\n",
    "for k, node in enumerate(['z_car'+str(i+1) for i in classK]):\n",
    "    plot_icar = ground_truth_2RasterCrs_concat_crop_exploded_stats_2point.loc[X_test.index,:].copy()\n",
    "    icar = z_cars_arr_test[:,k]\n",
    "    plot_icar[node] = icar\n",
    "    ax = plt.subplot(3,4,k+1)\n",
    "    divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', '5%', pad='3%')\n",
    "    img = plot_icar.plot(column=node, cmap='RdYlBu_r', edgecolor='k', legend=True, ax=ax, cax=cax, linewidth=0.05, markersize=1)\n",
    "    cax.tick_params(labelsize='4', width=0.4, length=5)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=4)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=4)\n",
    "    ax.set_title(node+' '+area_use_categories_le[k], fontsize=4)\n",
    "    re_shape_tsukuba_mirai_2RasterCrs.plot(facecolor='none', edgecolor='k', alpha=0.8, ax=ax, linewidth=0.2)\n",
    "    ax.yaxis.offsetText.set_fontsize(4)\n",
    "    ax.yaxis.set_major_formatter(ptick.ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure().clear()\n",
    "plt.close()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "del plot_icar, fig\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa50d4-d807-4b7b-a414-91a80220b0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e156aa8-e45e-4b7a-834a-372916c1f26f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LightGBM学習（OptunaのLightGBMTunerCV使用）\n",
    "params = {'task': 'train',\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'multiclass',\n",
    "          'metric': 'multi_logloss',\n",
    "          'verbose': -1,\n",
    "          'num_class': len(classK),\n",
    "          'random_state': 0,  # 乱数シード\n",
    "         }\n",
    "\n",
    "lgb_train = opt_lgb.Dataset(X_train[explanatory_variables], y_train[objective_variables], weight=sample_ws)\n",
    "skf = sklearn.model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# LightGBM学習\n",
    "tuner_cv = opt_lgb.LightGBMTunerCV(params, lgb_train\n",
    "                                   , num_boost_round=1000\n",
    "                                   , folds=skf\n",
    "                                   , return_cvbooster=True\n",
    "                                   , optuna_seed=0\n",
    "                                   , callbacks=[opt_lgb.early_stopping(stopping_rounds=50, verbose=True)])\n",
    "\n",
    "# 最適なパラメータを探索する\n",
    "tuner_cv.run()\n",
    "# 最も良かったスコアとパラメータを書き出す\n",
    "print(f'Best score: {tuner_cv.best_score}')\n",
    "print('Best params:')\n",
    "print(tuner_cv.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fd887-2c3c-4c40-8f40-cab65cd60d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBMTunerCVでモデル作成時、n_splits数だけモデルができる\n",
    "# すべてのモデルの結果の平均をとる関数\n",
    "def cv_model_output(models, X_test):\n",
    "    preds = []\n",
    "    for mdl in models:\n",
    "        pred = mdl.predict(X_test)\n",
    "        preds.append(pred)\n",
    "    pred = np.mean(np.array(preds), axis=0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004df5de-8d2f-442c-abc4-a37cb19c1348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データ推論\n",
    "models = tuner_cv.get_best_booster()\n",
    "models = models.boosters\n",
    "y_pred = cv_model_output(models, X_train[explanatory_variables].to_numpy())\n",
    "m_class = y_pred.argmax(axis=1)  # 最も確率が高いクラスを所属クラスとする\n",
    "print(sklearn.metrics.classification_report(y_train[objective_variables].to_numpy(), m_class.ravel()), '\\n')\n",
    "cm = sklearn.metrics.confusion_matrix(y_train[objective_variables].to_numpy(), m_class.ravel())\n",
    "print(cm, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79520444-b267-42d0-b2b1-59486e1a605e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 学習データ推論結果可視化\n",
    "results = y_train.copy()\n",
    "results['pred'] = m_class.ravel()\n",
    "results['diff'] = results[objective_variables]-results['pred']\n",
    "results.loc[(results['diff']==0), 'diff'] = 0\n",
    "results.loc[(results['diff']!=0), 'diff'] = 1\n",
    "point_plot(results, re_shape_tsukuba_mirai_2RasterCrs, linewidth=0.05, markersize=5, title='土壌分類', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b05f23-d419-42a6-a352-7ea705d70aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータ推論\n",
    "y_pred = cv_model_output(models, X_test[explanatory_variables].to_numpy())\n",
    "m_class = y_pred.argmax(axis=1)  # 最も確率が高いクラスを所属クラスとする\n",
    "print(sklearn.metrics.classification_report(y_test[objective_variables].to_numpy(), m_class.ravel()), '\\n')\n",
    "cm = sklearn.metrics.confusion_matrix(y_test[objective_variables].to_numpy(), m_class.ravel())\n",
    "print(cm, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f32223-f011-4444-a124-3ce70b969b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータ推論結果可視化\n",
    "results = y_test.copy()\n",
    "results['pred'] = m_class.ravel()\n",
    "results['diff'] = results[objective_variables]-results['pred']\n",
    "results.loc[(results['diff']==0), 'diff'] = 0\n",
    "results.loc[(results['diff']!=0), 'diff'] = 1\n",
    "point_plot(results, re_shape_tsukuba_mirai_2RasterCrs, linewidth=0.05, markersize=1, title='土壌分類', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa4d7cf-37de-4ee9-9b68-4ebe5cce3cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
